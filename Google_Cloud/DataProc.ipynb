{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataproc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Cloud Tutorial - PySpark and DataProc"
      ],
      "metadata": {
        "id": "W_0EPjG1hxpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "W4fT4iqRh9G7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume that GC SDK is installed ([tutorial](https://cloud.google.com/sdk/docs/install)). After creating a project (i.e., gcloud projects create PROJECT_NAME e.g., ex-dataproc), we create a cluster in dataproc: \n",
        "\n",
        "!gcloud dataproc clusters create CLUSTER_NAME (e.g., ex-dataproc)\n",
        "* --enable-component-gateway (access to the web interfaces of default and selected optional components (e.g., Jupyter) on the cluster.)\n",
        "* --region REGION_NAME --zone ZONE_NAME\n",
        "* --master-machine-type\n",
        "* --num-workers 2 (i.e., 2 worker machines)\n",
        "* --optional-components JUPYTER (several other options: anaconda, Docker, Solr)\n",
        "* --project PROJECT_NAME (i.e., ex-dataproc)\n"
      ],
      "metadata": {
        "id": "xs525Zcx1dtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create ex-dataproc --enable-component-gateway --region us-central1 --zone us-central1-c --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --optional-components JUPYTER --project dataproc-334718"
      ],
      "metadata": {
        "id": "eOTQftJHiPal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating cluster, there are one master machine (ex-dataproc-m) and two workers (ex-dataproc-w-0 and ex-dataproc-w-1). We can create an SSH tunnel using local port (e.g., 1080) to connect to a web interface (using Chrome)."
      ],
      "metadata": {
        "id": "UkxAeyFC49gE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I4G80p3fL27"
      },
      "outputs": [],
      "source": [
        "!gcloud compute ssh ex-dataproc-m --project=dataproc-334718 --zone=us-central1-c -- -D 1080 -N\n",
        "\n",
        "\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\" --proxy-server=\"socks5://localhost:1080\" --user-data-dir=\"/tmp/ex-dataproc-m\" http://ex-dataproc-m:8088"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a storage bucket (e.g., ex-dataproc-bucket) to keep all of our data (e.g., csv files). See this [tutorial](https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-console).\n",
        "\n",
        "gsutil mb -p PROJECT_ID -c STORAGE_CLASS -l BUCKET_LOCATION -b on gs://BUCKET_NAME"
      ],
      "metadata": {
        "id": "Fnm777PA6SWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil mb -p dataproc-334718 -b on gs://ex-dataproc-bucket"
      ],
      "metadata": {
        "id": "5xuV5QZE8gop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload data"
      ],
      "metadata": {
        "id": "CTp2iByj9-Ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a [dataset](https://www.kaggle.com/c/nlp-getting-started) of Kaggle."
      ],
      "metadata": {
        "id": "5A7EQMNEBfle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp nlpDisasterTweets.csv gs://ex-dataproc-bucket"
      ],
      "metadata": {
        "id": "ynElayaW-iQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the case we have multiple files such as data-1.csv, data-2.csv. You put all these files into a folder (e.g., /data). The location of this dataset is \"gs://ex-dataproc-bucket/data\""
      ],
      "metadata": {
        "id": "IwpteT1sByby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work like a charm"
      ],
      "metadata": {
        "id": "bVOVv4HAAvzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f6250c7",
        "outputId": "0b66048a-9657-4e71-bd68-9807b0794dd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "data = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(\"gs://ex-dataproc-bucket/nlpDisasterTweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7e0fd81",
        "outputId": "7853f8dd-0a32-485a-b007-10cdadf03c71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of row in Data: 7613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print('Number of row in Data:', data.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ad9b983",
        "outputId": "f60919e5-a794-4929-f2c6-d7376c7d175b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+--------+--------------------+------+\n",
            "| id|keyword|location|                text|target|\n",
            "+---+-------+--------+--------------------+------+\n",
            "|  1|   null|    null|Our Deeds are the...|     1|\n",
            "|  4|   null|    null|Forest fire near ...|     1|\n",
            "|  5|   null|    null|All residents ask...|     1|\n",
            "|  6|   null|    null|13,000 people rec...|     1|\n",
            "|  7|   null|    null|Just got sent thi...|     1|\n",
            "+---+-------+--------+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show(5)"
      ]
    }
  ]
}