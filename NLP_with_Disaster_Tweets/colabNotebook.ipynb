{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.7/dist-packages/pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print('SparkContext is not created!')\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "print(sc, sc.version)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true').load(trainPath)\n",
    "trainData.createOrReplaceTempView('trainData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = 'nlp-getting-started/test.csv'\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true').load(testPath)\n",
    "testData.createOrReplaceTempView('testData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.na.fill(value=\"\",subset=[self.getInputCol()])\n",
    "        return dataset\n",
    "    \n",
    "class TextNoLinkTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TextNoLinkTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = r'(https?://\\S+)'\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class ContainLinkTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ContainLinkTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = r'(https?://\\S+)'\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1).otherwise(0))\n",
    "        return dataset\n",
    "    \n",
    "class KeywordLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(KeywordLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.length(self.getInputCol()))\n",
    "        return dataset\n",
    "    \n",
    "class TextNoLinkLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TextNoLinkLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.length(self.getInputCol()))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFillNanTransformer = FillNanTransformer(inputCol=\"text\")\n",
    "keywordFillNanTransformer = FillNanTransformer(inputCol=\"keyword\")\n",
    "locationFillNanTransformer = FillNanTransformer(inputCol=\"location\")\n",
    "\n",
    "containLinkTransformer = ContainLinkTransformer(inputCol=\"text\", outputCol=\"containLink\")\n",
    "textNoLinkTransformer = TextNoLinkTransformer(inputCol=\"text\", outputCol=\"textNoLink\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoLink\", outputCol=\"wordText\", pattern=\"\\\\W\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"wordText\", outputCol=\"wordTextNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"wordTextNoSW\", outputCol=\"vecText\")\n",
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\")\n",
    "\n",
    "keywordLengthTransformer = KeywordLengthTransformer(inputCol=\"keyword\", outputCol=\"keywordLength\")\n",
    "textNoLinkLengthTransformer = TextNoLinkLengthTransformer(inputCol=\"textNoLink\", outputCol=\"textNoLinkLength\")\n",
    "\n",
    "catAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\"], outputCol=\"catFeatures\")\n",
    "\n",
    "catTextAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\", \"vecText\"], outputCol=\"catTextFeatures\")\n",
    "\n",
    "catFeatureRobustScaler = RobustScaler(inputCol=\"catFeatures\", outputCol=\"catFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)\n",
    "\n",
    "catTextFeatureRobustScaler = RobustScaler(inputCol=\"catTextFeatures\", outputCol=\"catTextFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[textFillNanTransformer,\n",
    "                                keywordFillNanTransformer,\n",
    "                                locationFillNanTransformer,\n",
    "                                containLinkTransformer,\n",
    "                                textNoLinkTransformer,\n",
    "                                regexTokenizer,\n",
    "                                stopWordsRemover,\n",
    "                                word2Vec,\n",
    "                                keywordIndexer,\n",
    "                                locationIndexer,\n",
    "                                keywordLengthTransformer,\n",
    "                                textNoLinkLengthTransformer,\n",
    "                                catAssembler,\n",
    "                                catTextAssembler,\n",
    "                                catFeatureRobustScaler,\n",
    "                                catTextFeatureRobustScaler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = spark.sql(\"SELECT * FROM trainData WHERE target is not NULL\")  \n",
    "testData = spark.sql(\"SELECT * FROM testData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataPreprocessed = preprocessingPipeline.fit(trainingData).transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainingDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"catTextFeaturesRobustScaler\"\n",
    "labelCol = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,regParam = 0.1, maxIter=100),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=7),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=16, numTrees=15),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=20, maxDepth=16, stepSize=0.001),\n",
    "                 #\"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=2, layers=[5, 5, 2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol, maxIter=100, regParam=0.1)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, algorithm in zip(algorithmList.keys(), algorithmList.values()):\n",
    "    model = algorithm.fit(trainSet)\n",
    "    prediction = model.transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithmName = \"GBTC\"\n",
    "algorithm = algorithmList[algorithmName]\n",
    "prediction = algorithm.fit(trainSet).transform(validSet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
