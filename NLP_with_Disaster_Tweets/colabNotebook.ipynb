{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.7/dist-packages/pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(sc, sc.version, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = '/content/drive/MyDrive/Colab\\ Notebooks/Projects/kaggle/NLP_with_Disaster_Tweets/nlp-getting-started/train.csv'\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = '/content/drive/MyDrive/Colab\\ Notebooks/Projects/kaggle/NLP_with_Disaster_Tweets/nlp-getting-started/test.csv'\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset\n",
    "    \n",
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class CheckPatternTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(CheckPatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1.).otherwise(0.))\n",
    "        return dataset\n",
    "    \n",
    "class GetLengthTransformer(Transformer, HasInputCols, HasOutputCols):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(GetLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        for inputCol, outputCol in zip(self.getInputCols(), self.getOutputCols()):\n",
    "            dataset = dataset.withColumn(outputCol, F.length(inputCol))\n",
    "        return dataset\n",
    "    \n",
    "class ConcatenateTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCol=None):\n",
    "        super(ConcatenateTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCols()[0]))\n",
    "        for colName in self.getInputCols()[1:]:\n",
    "            dataset = dataset.withColumn(self.getOutputCol(), \n",
    "                F.concat_ws('@', F.col(self.getOutputCol()), F.col(colName)))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillNanTransformer = FillNanTransformer(inputCols=[\"keyword\", \"location\"], nanReplacement=\"$\")\n",
    "textFillNanTransformer = FillNanTransformer(inputCols=[\"text\"], nanReplacement=\"\")\n",
    "\n",
    "#---\n",
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")\n",
    "\n",
    "#---\n",
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")\n",
    "checkUrlTransformer = CheckPatternTransformer(inputCol=\"text\", outputCol=\"textIsContainedUrl\", pattern=\"(https?://\\S+)\")\n",
    "\n",
    "getLengthTransformer = GetLengthTransformer(inputCols=[\"keyword\",\"textNoUrl\"], outputCols=[\"keywordLen\", \"textNoUrlLen\"])\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\"],\n",
    "                              outputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\"],\n",
    "                              handleInvalid=\"keep\")\n",
    "\n",
    "#---\n",
    "concatStringTransformer = ConcatenateTransformer(inputCols=[\"keyword\", \"location\", \"textNoUrl\"], outputCol=\"concatString\")\n",
    "concatStringRegexTokenizer = RegexTokenizer(inputCol=\"concatString\", outputCol=\"concatStringArrayWord\", pattern=\"\\\\W\")\n",
    "concatStringStopWordsRemover = StopWordsRemover(inputCol=\"concatStringArrayWord\", outputCol=\"concatStringArrayWordNoSW\")\n",
    "\n",
    "concatStringWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWord\", outputCol=\"concatStringVec\")\n",
    "concatStringNoSWWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWordNoSW\", outputCol=\"concatStringNoSWVec\")\n",
    "\n",
    "#---\n",
    "\n",
    "discreteFeaturesAssembler = VectorAssembler(inputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\",\n",
    "                                                      \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                            outputCol=\"discreteFeatures\")\n",
    "\n",
    "discreteAndTextFeaturesAssembler = VectorAssembler(inputCols=[\"discreteFeatures\", \"textVec\"],\n",
    "                                                   outputCol=\"discreteAndTextFeatures\")\n",
    "\n",
    "discreteFeaturesRobustScaler = RobustScaler(inputCol=\"discreteFeatures\", outputCol=\"discreteFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)\n",
    "\n",
    "discreteAndTextFeaturesRobustScaler = RobustScaler(inputCol=\"discreteAndTextFeatures\", outputCol=\"discreteAndTextFeaturesScale\",\n",
    "                                                   withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[fillNanTransformer, textFillNanTransformer,\n",
    "                                         removeUrlTransformer, regexTokenizer, stopWordsRemover, word2Vec,\n",
    "                                         keywordIndexer, locationIndexer, checkUrlTransformer, getLengthTransformer, oneHotEncoder,\n",
    "                                         concatStringTransformer, concatStringRegexTokenizer, concatStringStopWordsRemover, \n",
    "                                         concatStringWord2Vec, concatStringNoSWWord2Vec,\n",
    "                                         discreteFeaturesAssembler, \n",
    "                                         discreteAndTextFeaturesAssembler,\n",
    "                                         discreteFeaturesRobustScaler, discreteAndTextFeaturesRobustScaler\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"concatStringNoSWVec\"\n",
    "labelCol = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,regParam = 0.1, maxIter=50),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5, numTrees=20),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, maxDepth=5, stepSize=0.1),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, layers=[50, 5, 2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, regParam=0.01)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, algorithm in zip(algorithmList.keys(), algorithmList.values()):\n",
    "    startTime = time.time()\n",
    "    model = algorithm.fit(trainSet)\n",
    "    prediction = model.transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'{name:4}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
