{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.7/dist-packages/pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml:0.9.1\") \\\n",
    "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = \"/content/drive/MyDrive/Colab Notebooks/Projects/kaggle/NLP_with_Disaster_Tweets/nlp-getting-started/train.csv\"\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = \"/content/drive/MyDrive/Colab\\ Notebooks/Projects/kaggle/NLP_with_Disaster_Tweets/nlp-getting-started/test.csv\"\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler\n",
    "\n",
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")\n",
    "\n",
    "preprocessingPipeline = Pipeline(stages=[removeUrlTransformer, \n",
    "                                         regexTokenizer, stopWordsRemover, word2Vec \n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCol = \"target\"\n",
    "featuresCol = \"textVec\"\n",
    "\n",
    "lgbmc = LightGBMClassifier(boostingType='dart',\n",
    "                           objective= 'binary',\n",
    "                           metric= 'auc',\n",
    "                           isUnbalance= True,\n",
    "                           numIterations= 300,\n",
    "                           labelCol=\"target\",\n",
    "                           featuresCol=\"textVec\")\n",
    "\n",
    "prediction = lgbmc.fit(trainSet).transform(validSet)\n",
    "print(evaluator.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.automl import *\n",
    "from synapse.ml.train import *\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "\n",
    "trainSetAllHP = (trainDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "trainSetHP = (trainSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "validSetHP = (validSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "testSetHP = (testDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "\n",
    "# We remove \"[]\" in the column names.\n",
    "trainSetAllHP = trainSetAllHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in trainSetAllHP.columns])\n",
    "trainSetHP = trainSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in trainSetHP.columns])\n",
    "validSetHP = validSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in validSetHP.columns])\n",
    "testSetHP = testSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in testSetHP.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.automl import *\n",
    "from synapse.ml.train import *\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lgbmc = LightGBMClassifier(boostingType='dart',\n",
    "                           objective= 'binary',\n",
    "                           metric= 'auc',\n",
    "                           isUnbalance= True,\n",
    "                           numIterations= 300)\n",
    "\n",
    "smlmodels = [lgbmc]\n",
    "mmlmodels = [TrainClassifier(model=model, labelCol=labelCol) for model in smlmodels]\n",
    "\n",
    "paramBuilder = (HyperparamBuilder()\n",
    ".addHyperparam(lgbmc, lgbmc.learningRate, RangeHyperParam(0.01, 0.5))\n",
    ".addHyperparam(lgbmc, lgbmc.maxDepth, DiscreteHyperParam([1,30]))\n",
    ".addHyperparam(lgbmc, lgbmc.numLeaves, DiscreteHyperParam([10,200]))\n",
    ".addHyperparam(lgbmc, lgbmc.featureFraction, RangeHyperParam(0.1, 1.0))\n",
    ".addHyperparam(lgbmc, lgbmc.baggingFraction, RangeHyperParam(0.1, 1.0))\n",
    ".addHyperparam(lgbmc, lgbmc.baggingFreq, RangeHyperParam(0, 3))\n",
    ")\n",
    "\n",
    "searchSpace = paramBuilder.build()\n",
    "\n",
    "randomSpace = RandomSpace(searchSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = TuneHyperparameters(evaluationMetric=\"AUC\", models=mmlmodels, numFolds=2, \n",
    "                                numRuns=len(mmlmodels) * 2, parallelism=1, \n",
    "                                paramSpace=randomSpace.space(), seed=0).fit(trainSetHP)\n",
    "\n",
    "prediction = bestModel.transform(validSetHP)\n",
    "predLabel = np.array(prediction.select('scored_labels').collect()).squeeze()\n",
    "trueLabel = np.array(prediction.select('target').collect()).squeeze()\n",
    "print(metrics.roc_auc_score(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "    name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"LightGbmTunning\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
