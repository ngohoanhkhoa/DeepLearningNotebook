{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'nlp-getting-started/train.csv'\n",
    "data = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNonNull = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Columns\":20}: {\"All\":10} {\"NonNull\":10} {\"%NonNull\":10} {\"Difference\"}')\n",
    "for idx, col in enumerate(data.columns):\n",
    "    allValue = data.count()[idx]\n",
    "    nonNullValue = dataNonNull.count()[idx]\n",
    "    per = nonNullValue*100/allValue\n",
    "    diff = allValue - nonNullValue\n",
    "    print(f'{col:20}: {allValue} {nonNullValue:10} {np.round(per):10} {diff:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'].fillna(\"\", inplace=True)\n",
    "data['location'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'target'\n",
    "\n",
    "#----------------------\n",
    "def getCategoricalColumn(value):\n",
    "    if value == 1: return \"Disaster\"\n",
    "    else: return \"Not disaster\"\n",
    "    \n",
    "CategoricalColumn = data[columnName].apply(getCategoricalColumn)\n",
    "CategoricalColumn.name = 'catTarget'\n",
    "\n",
    "df = pd.concat([data, CategoricalColumn], axis=1)\n",
    "#----------------------\n",
    "\n",
    "groups = []\n",
    "for group, subset in df.groupby(by=CategoricalColumn.name):\n",
    "    groups.append({\n",
    "        CategoricalColumn.name: group,\n",
    "        'Count': len(subset)\n",
    "    })\n",
    "\n",
    "lenData = data[columnName].count()\n",
    "\n",
    "dataCategoricalQuality = pd.DataFrame(groups)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "dataCategoricalQuality.plot.bar(x=CategoricalColumn.name, ax=ax)\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    value = str(groups[i]['Count'])+': '+str(np.round(groups[i]['Count']*100/lenData))+'%'\n",
    "    ax.text(i, groups[i]['Count'], value , horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "ax.set_ylim(0, lenData - lenData/5)\n",
    "\n",
    "ax.set_xlabel('target')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sum: '+ str(lenData) )\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].str.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keyword'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target vs keyword length (character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywordLengthChar'] = data['keyword'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that keyword lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'location'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of character (including space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLengthChar'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthChar'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that text lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordTextLength(text):\n",
    "    return len(text.split())\n",
    "data['textLengthWord'] = data['text'].apply(getWordTextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthWord'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that number of words in text are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://\\S+)'\n",
    "data['link']= data[\"text\"].str.extract(pattern)\n",
    "    \n",
    "data['containLink'] = data['link'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'containLink'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)\n",
    "crossTable.rename(index={False : 'No link',True : 'Link',}, inplace=True)\n",
    "\n",
    "crossTable['Not disaster %'] = crossTable['Not disaster'] * 100 / crossTable['All']\n",
    "crossTable['Disaster %'] = crossTable['Disaster'] * 100 / crossTable['All']\n",
    "\n",
    "crossTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are more links in Disaster than Not Disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing data using PyPark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(sc, sc.version, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row in Training: 7613\n",
      "Number of row in Test:     3263\n"
     ]
    }
   ],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "testPath = 'nlp-getting-started/test.csv'\n",
    "\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)\n",
    "\n",
    "print('Number of row in Training:', trainData.count())\n",
    "print('Number of row in Test:    ', testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset\n",
    "    \n",
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class CheckPatternTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(CheckPatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1.).otherwise(0.))\n",
    "        return dataset\n",
    "    \n",
    "class GetLengthTransformer(Transformer, HasInputCols, HasOutputCols):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(GetLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        for inputCol, outputCol in zip(self.getInputCols(), self.getOutputCols()):\n",
    "            dataset = dataset.withColumn(outputCol, F.length(inputCol))\n",
    "        return dataset\n",
    "    \n",
    "class ConcatenateTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCol=None):\n",
    "        super(ConcatenateTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCols()[0]))\n",
    "        for colName in self.getInputCols()[1:]:\n",
    "            dataset = dataset.withColumn(self.getOutputCol(), \n",
    "                F.concat_ws('@', F.col(self.getOutputCol()), F.col(colName)))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillNanTransformer = FillNanTransformer(inputCols=[\"keyword\", \"location\"], nanReplacement=\"$\")\n",
    "textFillNanTransformer = FillNanTransformer(inputCols=[\"text\"], nanReplacement=\"\")\n",
    "\n",
    "#---\n",
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")\n",
    "\n",
    "#---\n",
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")\n",
    "checkUrlTransformer = CheckPatternTransformer(inputCol=\"text\", outputCol=\"textIsContainedUrl\", pattern=\"(https?://\\S+)\")\n",
    "\n",
    "getLengthTransformer = GetLengthTransformer(inputCols=[\"keyword\",\"textNoUrl\"], outputCols=[\"keywordLen\", \"textNoUrlLen\"])\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\"],\n",
    "                              outputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\"],\n",
    "                              handleInvalid=\"keep\")\n",
    "\n",
    "#---\n",
    "concatStringTransformer = ConcatenateTransformer(inputCols=[\"keyword\", \"location\", \"textNoUrl\"], outputCol=\"concatString\")\n",
    "concatStringRegexTokenizer = RegexTokenizer(inputCol=\"concatString\", outputCol=\"concatStringArrayWord\", pattern=\"\\\\W\")\n",
    "concatStringStopWordsRemover = StopWordsRemover(inputCol=\"concatStringArrayWord\", outputCol=\"concatStringArrayWordNoSW\")\n",
    "\n",
    "concatStringWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWord\", outputCol=\"concatStringVec\")\n",
    "concatStringNoSWWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWordNoSW\", outputCol=\"concatStringNoSWVec\")\n",
    "\n",
    "#---\n",
    "\n",
    "discreteFeaturesAssembler = VectorAssembler(inputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\",\n",
    "                                                      \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                            outputCol=\"discreteFeatures\")\n",
    "\n",
    "discreteAndTextFeaturesAssembler = VectorAssembler(inputCols=[\"discreteFeatures\", \"textVec\"],\n",
    "                                                   outputCol=\"discreteAndTextFeatures\")\n",
    "\n",
    "discreteFeaturesRobustScaler = RobustScaler(inputCol=\"discreteFeatures\", outputCol=\"discreteFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)\n",
    "\n",
    "discreteAndTextFeaturesRobustScaler = RobustScaler(inputCol=\"discreteAndTextFeatures\", outputCol=\"discreteAndTextFeaturesScale\",\n",
    "                                                   withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[fillNanTransformer, textFillNanTransformer,\n",
    "                                         removeUrlTransformer, regexTokenizer, stopWordsRemover, word2Vec,\n",
    "                                         keywordIndexer, locationIndexer, checkUrlTransformer, getLengthTransformer, oneHotEncoder,\n",
    "                                         concatStringTransformer, concatStringRegexTokenizer, concatStringStopWordsRemover, \n",
    "                                         concatStringWord2Vec, concatStringNoSWWord2Vec,\n",
    "                                         discreteFeaturesAssembler, \n",
    "                                         discreteAndTextFeaturesAssembler,\n",
    "                                         discreteFeaturesRobustScaler, discreteAndTextFeaturesRobustScaler\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"concatStringNoSWVec\"\n",
    "labelCol = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=labelCol, rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,regParam = 0.1, maxIter=50),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5, numTrees=20),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, maxDepth=5, stepSize=0.1),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=40, layers=[50, 20, 5, 2], stepSize=0.005, ),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, regParam=0.01)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe feature importance of categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywordIndex        : 0.19217661119128776\n",
      "locationIndex       : 0.013495560468460904\n",
      "textIsContainedUrl  : 0.3849196318014386\n",
      "keywordLen          : 0.2706514888496067\n",
      "textNoUrlLen        : 0.13875670768920598\n"
     ]
    }
   ],
   "source": [
    "discreteIndexFeaturesAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\",\n",
    "                                                            \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                                 outputCol=\"discreteIndexFeatures\")\n",
    "\n",
    "discreteIndexFeaturesRobustScaler = RobustScaler(inputCol=\"discreteIndexFeatures\", outputCol=\"discreteIndexFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)\n",
    "\n",
    "checkSet = discreteIndexFeaturesAssembler.transform(trainSet)\n",
    "checkSet = discreteIndexFeaturesRobustScaler.fit(checkSet).transform(checkSet)\n",
    "\n",
    "featuresImportanceModel = RandomForestClassifier(featuresCol=\"discreteIndexFeaturesScale\", \n",
    "                                                 labelCol=labelCol, maxDepth=5, numTrees=20).fit(checkSet)\n",
    "\n",
    "for column in zip(discreteIndexFeaturesAssembler.getInputCols(), list(featuresImportanceModel.featureImportances)):\n",
    "     print(f\"{column[0]:20}: {column[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe predictions of validation set based on several feature sets:**\n",
    "* \"discreteFeaturesScale\": \"keywordIndex\", \"locationIndex\", \"keywordLength\", \"textNoLinkLength\", \"containLink\"\n",
    "* \"discreteAndTextFeaturesScale\": \"discreteFeaturesScale\" and \"vecText\"\n",
    "* \"vecText\" only\n",
    "* \"concatStringVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing Urls), then apply Word2Vec\n",
    "* \"concatStringNoSWVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing Urls and stopwords), then apply Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param discreteFeaturesScale: 0.62361 in 7.903s\n",
      "Param discreteAndTextFeaturesScale: 0.67168 in 8.037s\n",
      "Param textVec: 0.63521 in 7.037s\n",
      "Param concatStringVec: 0.66073 in 7.592s\n",
      "Param concatStringNoSWVec: 0.66723 in 7.423s\n"
     ]
    }
   ],
   "source": [
    "featuresCols = [\"discreteFeaturesScale\", \"discreteAndTextFeaturesScale\", \n",
    "                \"textVec\", \"concatStringVec\", \"concatStringNoSWVec\"]\n",
    "algorithmName = \"LR\"\n",
    "for param in featuresCols:\n",
    "    startTime = time.time()\n",
    "    algorithm = algorithmList[algorithmName].setFeaturesCol(param)\n",
    "    prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'Param {param}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We observe that \"concatStringNoSWVec\" (concatenating strings in \"keyword\", \"location\" and \"text\" without urls and stopwords) produces the best result, slightly better than \"discreteAndTextFeaturesScale\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We combine two models: one based on categorical features and another based on text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param discreteFeaturesScale: 0.62361\n",
      "Param textVec: 0.63521\n",
      "------------\n",
      "0.0   1.0   0.63521\n",
      "0.1   0.9   0.63702\n",
      "0.2   0.8   0.64126\n",
      "0.30  0.7   0.65187\n",
      "0.4   0.6   0.6567\n",
      "0.5   0.5   0.65061\n",
      "0.60  0.39  0.66211\n",
      "0.70  0.29  0.66631\n",
      "0.8   0.19  0.65893\n",
      "0.9   0.09  0.6364\n",
      "1.0   0.0   0.62361\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "algorithmName = \"LR\"\n",
    "featuresCols = [\"discreteFeaturesScale\", \"textVec\"]\n",
    "\n",
    "predProbAll = []\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "for param in featuresCols:\n",
    "    startTime = time.time()\n",
    "    algorithm = algorithmList[algorithmName].setFeaturesCol(param)\n",
    "    \n",
    "    prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "    \n",
    "    predProb = np.array(prediction.select('probability').collect()).squeeze()\n",
    "    \n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f\"Param {param:2}: {np.round(score,5)}\")\n",
    "    \n",
    "    predProbAll.append(predProb)\n",
    "        \n",
    "print(\"------------\")\n",
    "for weight in np.arange(0.0, 1.1, 0.1):\n",
    "    predProb = weight*predProbAll[0] + (1.- weight)*predProbAll[1]\n",
    "    predLabel = np.array([1 if x[1] >= 0.5 else 0 for x in (predProb)])\n",
    "    score = metrics.roc_auc_score(trueLabel, predLabel)\n",
    "    print(f\"{str(weight)[:4]:5} {str((1.- weight))[:4]:5} {np.round(score,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The combination \\[0.1, 0.3] gives the highest score, but it does not outperform \"concatStringNoSWVec\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array(prediction.select('prediction').collect()).squeeze()\n",
    "predProb = np.array(prediction.select('probability').collect()).squeeze()\n",
    "\n",
    "print(metrics.classification_report(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import re\n",
    "\n",
    "def getCSVFromPysparkPath(path):\n",
    "    filenames = next(walk(path), (None, None, []))[2]\n",
    "    for filename in filenames:\n",
    "        if re.search('^part', filename):\n",
    "            return path+'/'+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"discreteAndTextFeaturesScale\"\n",
    "labelCol = \"target\"\n",
    "numFeature = len(validSet.select(featuresCol).collect()[0][0])\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "(trainSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([\"target\"]+ [F.col(\"feature\")[i] for i in range(numFeature)]).write.mode('overwrite').csv('trainCSV')\n",
    "\n",
    "(validSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(numFeature)]).write.mode('overwrite').csv('validCSV')\n",
    "\n",
    "trainPath = getCSVFromPysparkPath(\"trainCSV\")\n",
    "validPath = getCSVFromPysparkPath(\"validCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "(testDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(numFeature)]).write.mode('overwrite').csv('testCSV')\n",
    "\n",
    "testPath = getCSVFromPysparkPath(\"testCSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetLgb = lgb.Dataset(trainPath)\n",
    "#validSetLgb = lgb.Dataset(validPath, reference=trainSetLgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves':32, \n",
    "         'objective': 'binary',\n",
    "         'metric': 'auc',\n",
    "         'learning_rate': 0.01,\n",
    "         'boosting': 'dart',\n",
    "         'label_column': 0,\n",
    "        }\n",
    "\n",
    "num_round=500\n",
    "modelLgb = lgb.train(param,trainSetLgb, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7282400158775664\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "predProbModel = modelLgb.predict(validPath, num_iteration=bst.best_iteration)\n",
    "\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array([1 if x >= 0.5 else 0 for x in predProbModel])\n",
    "predProb = np.array([[1. - x, x] for x in predProbModel])\n",
    "\n",
    "print(metrics.roc_auc_score(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.784364400265437\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvLgb = lgb.cv(param, trainSetLgb, 200, nfold=3)[\"auc-mean\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(cvLgb)\n",
    "print('Score:', np.mean(cvLgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict and write file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = algorithm.fit(trainDataPreprocessed).transform(testDataPreprocessed)\n",
    "predLabelTest = np.array(predictionTest.select('prediction').collect()).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = modelLgb.predict(testPath, num_iteration=bst.best_iteration)\n",
    "predLabelTest = np.array([1 if x >= 0.5 else 0 for x in predictionTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = submission['target'] + predLabelTest.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/ngohoanhkhoa/.kaggle/kaggle.json'\n",
      "100%|| 22.2k/22.2k [00:03<00:00, 6.90kB/s]\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"LightGbm-Concat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
