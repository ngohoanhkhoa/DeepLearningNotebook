{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'nlp-getting-started/train.csv'\n",
    "data = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNonNull = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Columns\":20}: {\"All\":10} {\"NonNull\":10} {\"%NonNull\":10} {\"Difference\"}')\n",
    "for idx, col in enumerate(data.columns):\n",
    "    allValue = data.count()[idx]\n",
    "    nonNullValue = dataNonNull.count()[idx]\n",
    "    per = nonNullValue*100/allValue\n",
    "    diff = allValue - nonNullValue\n",
    "    print(f'{col:20}: {allValue} {nonNullValue:10} {np.round(per):10} {diff:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'].fillna(\"\", inplace=True)\n",
    "data['location'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'target'\n",
    "\n",
    "#----------------------\n",
    "def getCategoricalColumn(value):\n",
    "    if value == 1: return \"Disaster\"\n",
    "    else: return \"Not disaster\"\n",
    "    \n",
    "CategoricalColumn = data[columnName].apply(getCategoricalColumn)\n",
    "CategoricalColumn.name = 'catTarget'\n",
    "\n",
    "df = pd.concat([data, CategoricalColumn], axis=1)\n",
    "#----------------------\n",
    "\n",
    "groups = []\n",
    "for group, subset in df.groupby(by=CategoricalColumn.name):\n",
    "    groups.append({\n",
    "        CategoricalColumn.name: group,\n",
    "        'Count': len(subset)\n",
    "    })\n",
    "\n",
    "lenData = data[columnName].count()\n",
    "\n",
    "dataCategoricalQuality = pd.DataFrame(groups)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "dataCategoricalQuality.plot.bar(x=CategoricalColumn.name, ax=ax)\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    value = str(groups[i]['Count'])+': '+str(np.round(groups[i]['Count']*100/lenData))+'%'\n",
    "    ax.text(i, groups[i]['Count'], value , horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "ax.set_ylim(0, lenData - lenData/5)\n",
    "\n",
    "ax.set_xlabel('target')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sum: '+ str(lenData) )\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].str.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keyword'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target vs keyword length (character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywordLengthChar'] = data['keyword'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that keyword lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'location'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of character (including space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLengthChar'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthChar'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that text lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordTextLength(text):\n",
    "    return len(text.split())\n",
    "data['textLengthWord'] = data['text'].apply(getWordTextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthWord'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that number of words in text are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://\\S+)'\n",
    "data['link']= data[\"text\"].str.extract(pattern)\n",
    "    \n",
    "data['containLink'] = data['link'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'containLink'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)\n",
    "crossTable.rename(index={False : 'No link',True : 'Link',}, inplace=True)\n",
    "\n",
    "crossTable['Not disaster %'] = crossTable['Not disaster'] * 100 / crossTable['All']\n",
    "crossTable['Disaster %'] = crossTable['Disaster'] * 100 / crossTable['All']\n",
    "\n",
    "crossTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are more links in Disaster than Not Disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing data using PyPark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(sc, sc.version, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row in Training: 7613\n",
      "Number of row in Test:     3263\n"
     ]
    }
   ],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "testPath = 'nlp-getting-started/test.csv'\n",
    "\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)\n",
    "\n",
    "print('Number of row in Training:', trainData.count())\n",
    "print('Number of row in Test:    ', testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset\n",
    "    \n",
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class CheckPatternTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(CheckPatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1.).otherwise(0.))\n",
    "        return dataset\n",
    "    \n",
    "class GetLengthTransformer(Transformer, HasInputCols, HasOutputCols):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(GetLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        for inputCol, outputCol in zip(self.getInputCols(), self.getOutputCols()):\n",
    "            dataset = dataset.withColumn(outputCol, F.length(inputCol))\n",
    "        return dataset\n",
    "    \n",
    "class ConcatenateTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCol=None):\n",
    "        super(ConcatenateTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCols()[0]))\n",
    "        for colName in self.getInputCols()[1:]:\n",
    "            dataset = dataset.withColumn(self.getOutputCol(), \n",
    "                F.concat_ws('@', F.col(self.getOutputCol()), F.col(colName)))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillNanTransformer = FillNanTransformer(inputCols=[\"keyword\", \"location\"], nanReplacement=\"$\")\n",
    "textFillNanTransformer = FillNanTransformer(inputCols=[\"text\"], nanReplacement=\"\")\n",
    "\n",
    "#---\n",
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")\n",
    "\n",
    "#---\n",
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")\n",
    "checkUrlTransformer = CheckPatternTransformer(inputCol=\"text\", outputCol=\"textIsContainedUrl\", pattern=\"(https?://\\S+)\")\n",
    "\n",
    "getLengthTransformer = GetLengthTransformer(inputCols=[\"keyword\",\"textNoUrl\"], outputCols=[\"keywordLen\", \"textNoUrlLen\"])\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\"],\n",
    "                              outputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\"],\n",
    "                              handleInvalid=\"keep\")\n",
    "\n",
    "#---\n",
    "concatStringTransformer = ConcatenateTransformer(inputCols=[\"keyword\", \"location\", \"textNoUrl\"], outputCol=\"concatString\")\n",
    "concatStringRegexTokenizer = RegexTokenizer(inputCol=\"concatString\", outputCol=\"concatStringArrayWord\", pattern=\"\\\\W\")\n",
    "concatStringStopWordsRemover = StopWordsRemover(inputCol=\"concatStringArrayWord\", outputCol=\"concatStringArrayWordNoSW\")\n",
    "\n",
    "concatStringWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWord\", outputCol=\"concatStringVec\")\n",
    "concatStringNoSWWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWordNoSW\", outputCol=\"concatStringNoSWVec\")\n",
    "\n",
    "#---\n",
    "\n",
    "discreteFeaturesAssembler = VectorAssembler(inputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\",\n",
    "                                                      \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                            outputCol=\"discreteFeatures\")\n",
    "\n",
    "discreteAndTextFeaturesAssembler = VectorAssembler(inputCols=[\"discreteFeatures\", \"textVec\"],\n",
    "                                                   outputCol=\"discreteAndTextFeatures\")\n",
    "\n",
    "discreteFeaturesRobustScaler = RobustScaler(inputCol=\"discreteFeatures\", outputCol=\"discreteFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)\n",
    "\n",
    "discreteAndTextFeaturesRobustScaler = RobustScaler(inputCol=\"discreteAndTextFeatures\", outputCol=\"discreteAndTextFeaturesScale\",\n",
    "                                                   withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[fillNanTransformer, textFillNanTransformer,\n",
    "                                         removeUrlTransformer, regexTokenizer, stopWordsRemover, word2Vec,\n",
    "                                         keywordIndexer, locationIndexer, checkUrlTransformer, getLengthTransformer, oneHotEncoder,\n",
    "                                         concatStringTransformer, concatStringRegexTokenizer, concatStringStopWordsRemover, \n",
    "                                         concatStringWord2Vec, concatStringNoSWWord2Vec,\n",
    "                                         discreteFeaturesAssembler, \n",
    "                                         discreteAndTextFeaturesAssembler,\n",
    "                                         discreteFeaturesRobustScaler, discreteAndTextFeaturesRobustScaler\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"concatStringNoSWVec\"\n",
    "labelCol = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=labelCol, rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,regParam = 0.1, maxIter=50),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5, numTrees=20),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, maxDepth=5, stepSize=0.1),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, layers=[50, 10, 2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, regParam=0.01)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe feature importance of categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywordIndex        : 0.19217661119128776\n",
      "locationIndex       : 0.013495560468460904\n",
      "textIsContainedUrl  : 0.3849196318014386\n",
      "keywordLen          : 0.2706514888496067\n",
      "textNoUrlLen        : 0.13875670768920598\n"
     ]
    }
   ],
   "source": [
    "discreteIndexFeaturesAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\",\n",
    "                                                            \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                                 outputCol=\"discreteIndexFeatures\")\n",
    "\n",
    "discreteIndexFeaturesRobustScaler = RobustScaler(inputCol=\"discreteIndexFeatures\", outputCol=\"discreteIndexFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)\n",
    "\n",
    "checkSet = discreteIndexFeaturesAssembler.transform(trainSet)\n",
    "checkSet = discreteIndexFeaturesRobustScaler.fit(checkSet).transform(checkSet)\n",
    "\n",
    "featuresImportanceModel = RandomForestClassifier(featuresCol=\"discreteIndexFeaturesScale\", \n",
    "                                                 labelCol=labelCol, maxDepth=5, numTrees=20).fit(checkSet)\n",
    "\n",
    "for column in zip(discreteIndexFeaturesAssembler.getInputCols(), list(featuresImportanceModel.featureImportances)):\n",
    "     print(f\"{column[0]:20}: {column[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe predictions of validation set based on several feature sets:**\n",
    "* \"discreteFeaturesScale\": \"keywordIndex\", \"locationIndex\", \"keywordLength\", \"textNoLinkLength\", \"containLink\"\n",
    "* \"discreteAndTextFeaturesScale\": \"discreteFeaturesScale\" and \"vecText\"\n",
    "* \"vecText\" only\n",
    "* \"concatStringVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing Urls), then apply Word2Vec\n",
    "* \"concatStringNoSWVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing Urls and stopwords), then apply Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param discreteFeaturesScale: 0.62361 in 7.712s\n",
      "Param discreteAndTextFeaturesScale: 0.67168 in 9.027s\n",
      "Param textVec: 0.63521 in 7.704s\n",
      "Param concatStringVec: 0.66073 in 7.864s\n",
      "Param concatStringNoSWVec: 0.66723 in 7.839s\n"
     ]
    }
   ],
   "source": [
    "featuresCols = [\"discreteFeaturesScale\", \"discreteAndTextFeaturesScale\", \n",
    "                \"textVec\", \"concatStringVec\", \"concatStringNoSWVec\"]\n",
    "algorithmName = \"LR\"\n",
    "for param in featuresCols:\n",
    "    startTime = time.time()\n",
    "    algorithm = algorithmList[algorithmName].setFeaturesCol(param)\n",
    "    prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'Param {param}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We observe that \"concatStringNoSWVec\" (concatenating strings in \"keyword\", \"location\" and \"text\" without urls and stopwords) produces the best result, slightly better than \"discreteAndTextFeaturesScale\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We combine two models: one based on categorical features and another based on text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param discreteFeaturesScale: 0.62361\n",
      "Param textVec: 0.63521\n",
      "------------\n",
      "0.0   1.0   0.63521\n",
      "0.1   0.9   0.63702\n",
      "0.2   0.8   0.64126\n",
      "0.30  0.7   0.65187\n",
      "0.4   0.6   0.6567\n",
      "0.5   0.5   0.65061\n",
      "0.60  0.39  0.66211\n",
      "0.70  0.29  0.66631\n",
      "0.8   0.19  0.65893\n",
      "0.9   0.09  0.6364\n",
      "1.0   0.0   0.62361\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "algorithmName = \"LR\"\n",
    "featuresCols = [\"discreteFeaturesScale\", \"textVec\"]\n",
    "\n",
    "predProbAll = []\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "for param in featuresCols:\n",
    "    startTime = time.time()\n",
    "    algorithm = algorithmList[algorithmName].setFeaturesCol(param)\n",
    "    \n",
    "    prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "    \n",
    "    predProb = np.array(prediction.select('probability').collect()).squeeze()\n",
    "    \n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f\"Param {param:2}: {np.round(score,5)}\")\n",
    "    \n",
    "    predProbAll.append(predProb)\n",
    "        \n",
    "print(\"------------\")\n",
    "for weight in np.arange(0.0, 1.1, 0.1):\n",
    "    predProb = weight*predProbAll[0] + (1.- weight)*predProbAll[1]\n",
    "    predLabel = np.array([1 if x[1] >= 0.5 else 0 for x in (predProb)])\n",
    "    score = metrics.roc_auc_score(trueLabel, predLabel)\n",
    "    print(f\"{str(weight)[:4]:5} {str((1.- weight))[:4]:5} {np.round(score,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The combination \\[0.1, 0.3] gives the highest score, but it does not outperform \"concatStringNoSWVec\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array(prediction.select('prediction').collect()).squeeze()\n",
    "predProb = np.array(prediction.select('probability').collect()).squeeze()\n",
    "\n",
    "print(metrics.classification_report(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import re\n",
    "\n",
    "def getCSVFromPysparkPath(path):\n",
    "    filenames = next(walk(path), (None, None, []))[2]\n",
    "    for filename in filenames:\n",
    "        if re.search('^part', filename):\n",
    "            return path+'/'+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"concatStringNoSWVec\"\n",
    "labelCol = \"target\"\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "(trainSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([\"target\"]+ [F.col(\"feature\")[i] for i in range(5)]).write.mode('overwrite').csv('trainCSV')\n",
    "\n",
    "(validSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(5)]).write.mode('overwrite').csv('validCSV')\n",
    "\n",
    "trainPath = getCSVFromPysparkPath(\"trainCSV\")\n",
    "validPath = getCSVFromPysparkPath(\"validCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "(testDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(50)]).write.mode('overwrite').csv('testCSV')\n",
    "\n",
    "testPath = getCSVFromPysparkPath(\"testCSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetLgb = lgb.Dataset(trainPath)\n",
    "#validSetLgb = lgb.Dataset(validPath, reference=trainSetLgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves':60, \n",
    "         'objective': 'binary',\n",
    "         'metric': 'auc',\n",
    "         'learning_rate': 0.01,\n",
    "         'boosting': 'dart',\n",
    "         'label_column': 0,\n",
    "        }\n",
    "\n",
    "num_round=500\n",
    "modelLgb = lgb.train(param,trainSetLgb, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6986570225152711\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "predProbModel = modelLgb.predict(validPath, num_iteration=bst.best_iteration)\n",
    "\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array([1 if x >= 0.5 else 0 for x in predProbModel])\n",
    "predProb = np.array([[1. - x, x] for x in predProbModel])\n",
    "\n",
    "print(metrics.roc_auc_score(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7103956764405998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvLgb = lgb.cv(param, trainSetLgb, 500, nfold=3)[\"auc-mean\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(cvLgb)\n",
    "print('Score:', np.mean(cvLgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict and write file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = algorithm.fit(trainDataPreprocessed).transform(testDataPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabelTest = np.array(predictionTest.select('prediction').collect()).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = modelLgb.predict(testPath, num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabelTest = np.array([1 if x >= 0.5 else 0 for x in predictionTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = submission['target'] + predLabelTest.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/ngohoanhkhoa/.kaggle/kaggle.json'\n",
      "100%|| 22.2k/22.2k [00:04<00:00, 4.82kB/s]\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"LightGbm-Concat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
