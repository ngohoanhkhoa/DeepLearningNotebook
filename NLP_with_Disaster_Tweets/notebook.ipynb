{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import mlflow\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'nlp-getting-started/train.csv'\n",
    "data = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNonNull = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Columns\":20}: {\"All\":10} {\"NonNull\":10} {\"%NonNull\":10} {\"Difference\"}')\n",
    "for idx, col in enumerate(data.columns):\n",
    "    allValue = data.count()[idx]\n",
    "    nonNullValue = dataNonNull.count()[idx]\n",
    "    per = nonNullValue*100/allValue\n",
    "    diff = allValue - nonNullValue\n",
    "    print(f'{col:20}: {allValue} {nonNullValue:10} {np.round(per):10} {diff:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'].fillna(\"\", inplace=True)\n",
    "data['location'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'target'\n",
    "\n",
    "#----------------------\n",
    "def getCategoricalColumn(value):\n",
    "    if value == 1: return \"Disaster\"\n",
    "    else: return \"Not disaster\"\n",
    "    \n",
    "CategoricalColumn = data[columnName].apply(getCategoricalColumn)\n",
    "CategoricalColumn.name = 'catTarget'\n",
    "\n",
    "df = pd.concat([data, CategoricalColumn], axis=1)\n",
    "#----------------------\n",
    "\n",
    "groups = []\n",
    "for group, subset in df.groupby(by=CategoricalColumn.name):\n",
    "    groups.append({\n",
    "        CategoricalColumn.name: group,\n",
    "        'Count': len(subset)\n",
    "    })\n",
    "\n",
    "lenData = data[columnName].count()\n",
    "\n",
    "dataCategoricalQuality = pd.DataFrame(groups)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "dataCategoricalQuality.plot.bar(x=CategoricalColumn.name, ax=ax)\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    value = str(groups[i]['Count'])+': '+str(np.round(groups[i]['Count']*100/lenData))+'%'\n",
    "    ax.text(i, groups[i]['Count'], value , horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "ax.set_ylim(0, lenData - lenData/5)\n",
    "\n",
    "ax.set_xlabel('target')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sum: '+ str(lenData) )\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].str.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keyword'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target vs keyword length (character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywordLengthChar'] = data['keyword'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that keyword lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'location'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of character (including space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLengthChar'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthChar'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that text lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordTextLength(text):\n",
    "    return len(text.split())\n",
    "data['textLengthWord'] = data['text'].apply(getWordTextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthWord'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that number of words in text are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://\\S+)'\n",
    "data['link']= data[\"text\"].str.extract(pattern)\n",
    "    \n",
    "data['containLink'] = data['link'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'containLink'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)\n",
    "crossTable.rename(index={False : 'No link',True : 'Link',}, inplace=True)\n",
    "\n",
    "crossTable['Not disaster %'] = crossTable['Not disaster'] * 100 / crossTable['All']\n",
    "crossTable['Disaster %'] = crossTable['Disaster'] * 100 / crossTable['All']\n",
    "\n",
    "crossTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are more links in Disaster than Not Disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing data using PyPark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=App> 3.2.0 <pyspark.sql.session.SparkSession object at 0x7f824179bc40>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(sc, sc.version, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row in Training: 7613\n",
      "Number of row in Test:     3263\n"
     ]
    }
   ],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "testPath = 'nlp-getting-started/test.csv'\n",
    "\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)\n",
    "\n",
    "print('Number of row in Training:', trainData.count())\n",
    "print('Number of row in Test:    ', testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols\n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.na.fill(value=\"\",subset=[self.getInputCol()])\n",
    "        return dataset\n",
    "    \n",
    "class TextNoLinkTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TextNoLinkTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = r'(https?://\\S+)'\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class ContainLinkTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ContainLinkTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = r'(https?://\\S+)'\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1).otherwise(0))\n",
    "        return dataset\n",
    "    \n",
    "class KeywordLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(KeywordLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.length(self.getInputCol()))\n",
    "        return dataset\n",
    "    \n",
    "class TextNoLinkLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TextNoLinkLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.length(self.getInputCol()))\n",
    "        return dataset\n",
    "    \n",
    "class ConcatenateTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCol=None):\n",
    "        super(ConcatenateTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCols()[0]))\n",
    "        for colName in self.getInputCols()[1:]:\n",
    "            dataset = dataset.withColumn(self.getOutputCol(), \n",
    "                F.concat_ws('@', F.col(self.getOutputCol()), F.col(colName)))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFillNanTransformer = FillNanTransformer(inputCol=\"text\")\n",
    "keywordFillNanTransformer = FillNanTransformer(inputCol=\"keyword\")\n",
    "locationFillNanTransformer = FillNanTransformer(inputCol=\"location\")\n",
    "\n",
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")\n",
    "\n",
    "containLinkTransformer = ContainLinkTransformer(inputCol=\"text\", outputCol=\"containLink\")\n",
    "textNoLinkTransformer = TextNoLinkTransformer(inputCol=\"text\", outputCol=\"textNoLink\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoLink\", outputCol=\"wordText\", pattern=\"\\\\W\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"wordText\", outputCol=\"wordTextNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"wordTextNoSW\", outputCol=\"vecText\")\n",
    "\n",
    "concatenateTransformer = ConcatenateTransformer(inputCols=[\"keyword\", \"location\", \"textNoLink\"], outputCol=\"concatText\")\n",
    "concatTextRegexTokenizer = RegexTokenizer(inputCol=\"concatText\", outputCol=\"wordConcatText\", pattern=\"\\\\W\")\n",
    "concatTextStopWordsRemover = StopWordsRemover(inputCol=\"wordConcatText\", outputCol=\"wordConcatTextNoSW\")\n",
    "concatTextWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"wordConcatTextNoSW\", outputCol=\"vecConcatText\")\n",
    "\n",
    "keywordLengthTransformer = KeywordLengthTransformer(inputCol=\"keyword\", outputCol=\"keywordLength\")\n",
    "textNoLinkLengthTransformer = TextNoLinkLengthTransformer(inputCol=\"textNoLink\", outputCol=\"textNoLinkLength\")\n",
    "\n",
    "catAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\"], outputCol=\"catFeatures\")\n",
    "\n",
    "catTextAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\", \"vecText\"], outputCol=\"catTextFeatures\")\n",
    "\n",
    "catFeatureRobustScaler = RobustScaler(inputCol=\"catFeatures\", outputCol=\"catFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)\n",
    "\n",
    "catTextFeatureRobustScaler = RobustScaler(inputCol=\"catTextFeatures\", outputCol=\"catTextFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)\n",
    "\n",
    "vecConcatTextRobustScaler = RobustScaler(inputCol=\"vecConcatText\", outputCol=\"vecConcatTextRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[textFillNanTransformer,\n",
    "                                keywordFillNanTransformer,\n",
    "                                locationFillNanTransformer,\n",
    "                                containLinkTransformer,\n",
    "                                textNoLinkTransformer,\n",
    "                                regexTokenizer,\n",
    "                                stopWordsRemover,\n",
    "                                word2Vec,\n",
    "                                keywordIndexer,\n",
    "                                locationIndexer,\n",
    "                                keywordLengthTransformer,\n",
    "                                textNoLinkLengthTransformer,\n",
    "                                catAssembler,\n",
    "                                catTextAssembler,\n",
    "                                catFeatureRobustScaler,\n",
    "                                catTextFeatureRobustScaler,\n",
    "                                concatenateTransformer,\n",
    "                                concatTextRegexTokenizer,\n",
    "                                concatTextStopWordsRemover,\n",
    "                                concatTextWord2Vec,\n",
    "                                vecConcatTextRobustScaler\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"vecConcatText\"\n",
    "labelCol = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,regParam = 0.1, maxIter=50),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5, numTrees=20),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, maxDepth=5, stepSize=0.1),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, layers=[50, 10, 2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, regParam=0.01)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe predictions of validation set based on several feature sets:\n",
    "* \"catFeaturesRobustScaler\": \"keywordIndex\", \"locationIndex\", \"keywordLength\", \"textNoLinkLength\", \"containLink\"\n",
    "* \"catTextFeaturesRobustScaler\": \"catFeaturesRobustScaler\" and \"vecText\"\n",
    "* \"vecText\" only\n",
    "* \"vecConcatText\": concatenate \"keyword\", \"location\" and \"textNoLink\", then apply Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param catFeaturesRobustScaler: 0.63477 in 6.302s\n",
      "Param catTextFeaturesRobustScaler: 0.71563 in 6.822s\n",
      "Param vecText: 0.70755 in 8.063s\n",
      "Param vecConcatText: 0.74124 in 6.694s\n"
     ]
    }
   ],
   "source": [
    "featuresCols = [\"catFeaturesRobustScaler\", \"catTextFeaturesRobustScaler\", \n",
    "                \"vecText\", \"vecConcatText\"]\n",
    "algorithmName = \"RFC\"\n",
    "for param in featuresCols:\n",
    "    startTime = time.time()\n",
    "    algorithm = algorithmList[algorithmName].setFeaturesCol(param)\n",
    "    prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'Param {param}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that concatenating strings in \"keyword\", \"location\" and \"text\" without links produce the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412398921832885\n"
     ]
    }
   ],
   "source": [
    "algorithmName = \"RFC\"\n",
    "algorithm = algorithmList[algorithmName]\n",
    "prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "score = evaluator.evaluate(prediction)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76       411\n",
      "           1       0.74      0.54      0.63       331\n",
      "\n",
      "    accuracy                           0.71       742\n",
      "   macro avg       0.72      0.69      0.69       742\n",
      "weighted avg       0.72      0.71      0.70       742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array(prediction.select('prediction').collect()).squeeze()\n",
    "predProb = np.array(prediction.select('probability').collect()).squeeze()\n",
    "\n",
    "print(metrics.classification_report(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import re\n",
    "\n",
    "def getCSVFromPysparkPath(path):\n",
    "    filenames = next(walk(path), (None, None, []))[2]\n",
    "    for filename in filenames:\n",
    "        if re.search('^part', filename):\n",
    "            return path+'/'+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"vecConcatText\"\n",
    "labelCol = \"target\"\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "(trainSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([\"target\"]+ [F.col(\"feature\")[i] for i in range(50)]).write.mode('overwrite').csv('trainCSV')\n",
    "\n",
    "(validSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(50)]).write.mode('overwrite').csv('validCSV')\n",
    "\n",
    "trainPath = getCSVFromPysparkPath(\"trainCSV\")\n",
    "validPath = getCSVFromPysparkPath(\"validCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "(testDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(50)]).write.mode('overwrite').csv('testCSV')\n",
    "\n",
    "testPath = getCSVFromPysparkPath(\"testCSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetLgb = lgb.Dataset(trainPath)\n",
    "#validSetLgb = lgb.Dataset(validPath, reference=trainSetLgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 5, \n",
    "         'objective': 'binary',\n",
    "         'metric': 'f1',\n",
    "         'learning_rate': 0.1,\n",
    "         'boosting': 'dart',\n",
    "         'label_column': 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round=100\n",
    "bst = lgb.train(param,trainSetLgb, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77       411\n",
      "           1       0.74      0.55      0.63       331\n",
      "\n",
      "    accuracy                           0.71       742\n",
      "   macro avg       0.72      0.70      0.70       742\n",
      "weighted avg       0.72      0.71      0.71       742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "predProbModel = bst.predict(validPath, num_iteration=bst.best_iteration)\n",
    "\n",
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array([1 if x >= 0.5 else 0 for x in predProbModel])\n",
    "predProb = np.array([[1. - x, x] for x in predProbModel])\n",
    "\n",
    "print(metrics.classification_report(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Predict and write file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = algorithm.fit(trainDataPreprocessed).transform(testDataPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabelTest = np.array(predictionTest.select('prediction').collect()).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = bst.predict(testPath, num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabelTest = np.array([1 if x >= 0.5 else 0 for x in predictionTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = submission['target'] + predLabelTest.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/ngohoanhkhoa/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 22.2k/22.2k [00:04<00:00, 4.82kB/s]\n",
      "Successfully submitted to Natural Language Processing with Disaster Tweets"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"LightGbm-Concat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
