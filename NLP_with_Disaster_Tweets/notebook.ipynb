{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'nlp-getting-started/train.csv'\n",
    "data = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNonNull = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Columns\":20}: {\"All\":10} {\"NonNull\":10} {\"%NonNull\":10} {\"Difference\"}')\n",
    "for idx, col in enumerate(data.columns):\n",
    "    allValue = data.count()[idx]\n",
    "    nonNullValue = dataNonNull.count()[idx]\n",
    "    per = nonNullValue*100/allValue\n",
    "    diff = allValue - nonNullValue\n",
    "    print(f'{col:20}: {allValue} {nonNullValue:10} {np.round(per):10} {diff:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'].fillna(\"\", inplace=True)\n",
    "data['location'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'target'\n",
    "\n",
    "#----------------------\n",
    "def getCategoricalColumn(value):\n",
    "    if value == 1: return \"Disaster\"\n",
    "    else: return \"Not disaster\"\n",
    "    \n",
    "CategoricalColumn = data[columnName].apply(getCategoricalColumn)\n",
    "CategoricalColumn.name = 'catTarget'\n",
    "\n",
    "df = pd.concat([data, CategoricalColumn], axis=1)\n",
    "#----------------------\n",
    "\n",
    "groups = []\n",
    "for group, subset in df.groupby(by=CategoricalColumn.name):\n",
    "    groups.append({\n",
    "        CategoricalColumn.name: group,\n",
    "        'Count': len(subset)\n",
    "    })\n",
    "\n",
    "lenData = data[columnName].count()\n",
    "\n",
    "dataCategoricalQuality = pd.DataFrame(groups)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "dataCategoricalQuality.plot.bar(x=CategoricalColumn.name, ax=ax)\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    value = str(groups[i]['Count'])+': '+str(np.round(groups[i]['Count']*100/lenData))+'%'\n",
    "    ax.text(i, groups[i]['Count'], value , horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "ax.set_ylim(0, lenData - lenData/5)\n",
    "\n",
    "ax.set_xlabel('target')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sum: '+ str(lenData) )\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].str.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keyword'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target vs keyword length (character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywordLengthChar'] = data['keyword'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that keyword lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'location'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of character (including space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLengthChar'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthChar'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that text lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordTextLength(text):\n",
    "    return len(text.split())\n",
    "data['textLengthWord'] = data['text'].apply(getWordTextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthWord'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that number of words in text are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://\\S+)'\n",
    "data['link']= data[\"text\"].str.extract(pattern)\n",
    "    \n",
    "data['containLink'] = data['link'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'containLink'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)\n",
    "crossTable.rename(index={False : 'No link',True : 'Link',}, inplace=True)\n",
    "\n",
    "crossTable['Not disaster %'] = crossTable['Not disaster'] * 100 / crossTable['All']\n",
    "crossTable['Disaster %'] = crossTable['Disaster'] * 100 / crossTable['All']\n",
    "\n",
    "crossTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are more links in Disaster than Not Disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing data using PyPark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(master = \"local\", appName = \"App\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(sc, sc.version, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)\n",
    "trainData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of row:', trainData.count())\n",
    "trainData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.createOrReplaceTempView('trainData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = 'nlp-getting-started/test.csv'\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)\n",
    "testData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of row:', testData.count())\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.createOrReplaceTempView('testData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preprocessing step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.sql(\"SELECT * FROM trainData WHERE target is not NULL\")\n",
    "dataframe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocessing Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.na.fill(value=\"\",subset=[\"keyword\", \"location\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textColector = dataframe.collect()\n",
    "for line in textColector[:5]:\n",
    "    print(line['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column checking if there is a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "pattern = r'(https?://\\S+)'\n",
    "\n",
    "dataframe = dataframe.withColumn(\"containLink\", F.when(F.col(\"text\").rlike(pattern),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column without link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://\\S+)'\n",
    "\n",
    "dataframe = dataframe.withColumn('textNoLink', F.regexp_replace(F.col('text'), pattern, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer: list of words and removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "dataframe = dataframe.drop('wordText')\n",
    "dataframe = RegexTokenizer(inputCol=\"textNoLink\", outputCol=\"wordText\", pattern=\"\\\\W\").transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "\n",
    "dataframe = dataframe.drop('wordTextNoSW')\n",
    "dataframe= StopWordsRemover(inputCol=\"wordText\", outputCol=\"wordTextNoSW\").transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "dataframe = dataframe.drop('vecText')\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"wordTextNoSW\", outputCol=\"vecText\")\n",
    "dataframe = word2Vec.fit(dataframe).transform(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocessing keyword and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# For mulitple columns\n",
    "dataframe = dataframe.drop('keywordIndex')\n",
    "dataframe = dataframe.drop('locationIndex')\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"Index\")\\\n",
    "            for column in list(set([\"keyword\", \"location\"])) ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "dataframe = pipeline.fit(dataframe).transform(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create several features based on length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn('keywordLength', F.length('keyword'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn('textNoLinkLength', F.length('textNoLink'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Feature assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataframe = dataframe.drop('catFeatures')\n",
    "catAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\"], outputCol=\"catFeatures\")\n",
    "\n",
    "dataframe = catAssembler.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop('catTextFeatures')\n",
    "catTextAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\", \"vecText\"], outputCol=\"catTextFeatures\")\n",
    "\n",
    "dataframe = catTextAssembler.transform(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RobustScaler\n",
    "\n",
    "dataframe = dataframe.drop('catFeaturesRobustScaler')\n",
    "catFeatureRobustScaler = RobustScaler(inputCol=\"catFeatures\", outputCol=\"catFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)\n",
    "\n",
    "dataframe = catFeatureRobustScaler.fit(dataframe).transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop('catTextFeaturesRobustScaler')\n",
    "catTextFeatureRobustScaler = RobustScaler(inputCol=\"catTextFeatures\", outputCol=\"catTextFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)\n",
    "\n",
    "dataframe = catTextFeatureRobustScaler.fit(dataframe).transform(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.na.fill(value=\"\",subset=[self.getInputCol()])\n",
    "        return dataset\n",
    "    \n",
    "class TextNoLinkTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TextNoLinkTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = r'(https?://\\S+)'\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class ContainLinkTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ContainLinkTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = r'(https?://\\S+)'\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1).otherwise(0))\n",
    "        return dataset\n",
    "    \n",
    "class KeywordLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(KeywordLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.length(self.getInputCol()))\n",
    "        return dataset\n",
    "    \n",
    "class TextNoLinkLengthTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TextNoLinkLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.length(self.getInputCol()))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFillNanTransformer = FillNanTransformer(inputCol=\"text\")\n",
    "keywordFillNanTransformer = FillNanTransformer(inputCol=\"keyword\")\n",
    "locationFillNanTransformer = FillNanTransformer(inputCol=\"location\")\n",
    "\n",
    "containLinkTransformer = ContainLinkTransformer(inputCol=\"text\", outputCol=\"containLink\")\n",
    "textNoLinkTransformer = TextNoLinkTransformer(inputCol=\"text\", outputCol=\"textNoLink\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoLink\", outputCol=\"wordText\", pattern=\"\\\\W\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"wordText\", outputCol=\"wordTextNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"wordTextNoSW\", outputCol=\"vecText\")\n",
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")\n",
    "\n",
    "keywordLengthTransformer = KeywordLengthTransformer(inputCol=\"keyword\", outputCol=\"keywordLength\")\n",
    "textNoLinkLengthTransformer = TextNoLinkLengthTransformer(inputCol=\"textNoLink\", outputCol=\"textNoLinkLength\")\n",
    "\n",
    "catAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\"], outputCol=\"catFeatures\")\n",
    "\n",
    "catTextAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \n",
    "                                        \"keywordLength\", \"textNoLinkLength\",\n",
    "                                        \"containLink\", \"vecText\"], outputCol=\"catTextFeatures\")\n",
    "\n",
    "catFeatureRobustScaler = RobustScaler(inputCol=\"catFeatures\", outputCol=\"catFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)\n",
    "\n",
    "catTextFeatureRobustScaler = RobustScaler(inputCol=\"catTextFeatures\", outputCol=\"catTextFeaturesRobustScaler\",\n",
    "                                      withScaling=True, withCentering=True,\n",
    "                                      lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[textFillNanTransformer,\n",
    "                                keywordFillNanTransformer,\n",
    "                                locationFillNanTransformer,\n",
    "                                containLinkTransformer,\n",
    "                                textNoLinkTransformer,\n",
    "                                regexTokenizer,\n",
    "                                stopWordsRemover,\n",
    "                                word2Vec,\n",
    "                                keywordIndexer,\n",
    "                                locationIndexer,\n",
    "                                keywordLengthTransformer,\n",
    "                                textNoLinkLengthTransformer,\n",
    "                                catAssembler,\n",
    "                                catTextAssembler,\n",
    "                                catFeatureRobustScaler,\n",
    "                                catTextFeatureRobustScaler\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = spark.sql(\"SELECT * FROM trainData\")  \n",
    "testData = spark.sql(\"SELECT * FROM testData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataPreprocessed = preprocessingPipeline.fit(trainingData).transform(trainingData)\n",
    "testDataPreprocessed = preprocessingPipeline.fit(trainingData).transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainingDataPreprocessed.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"catTextFeaturesRobustScaler\"\n",
    "labelCol = \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=labelCol, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol,regParam = 0.1, maxIter=50),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol, maxDepth=5, numTrees=20),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, maxDepth=5, stepSize=0.1),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, layers=[55, 5, 2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol, maxIter=50, regParam=0.01)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe predictions of validation set based on several feature sets:\n",
    "* \"catFeaturesRobustScaler\": \"keywordIndex\", \"locationIndex\", \"keywordLength\", \"textNoLinkLength\", \"containLink\"\n",
    "* \"catTextFeaturesRobustScaler\": \"catFeaturesRobustScaler\" and \"vecText\"\n",
    "* \"vecText\" only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols = [\"catFeaturesRobustScaler\", \"catTextFeaturesRobustScaler\", \"vecText\"]\n",
    "algorithmName = \"LR\"\n",
    "for param in featuresCols:\n",
    "    startTime = time.time()\n",
    "    algorithm = algorithmList[algorithmName].setFeaturesCol(param)\n",
    "    prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'Param {param}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithmName = \"MPC\"\n",
    "algorithm = algorithmList[algorithmName]\n",
    "prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "score = evaluator.evaluate(prediction)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import re\n",
    "\n",
    "def getCSVFromPysparkPath(path):\n",
    "    filenames = next(walk(path), (None, None, []))[2]\n",
    "    for filename in filenames:\n",
    "        if re.search('^part', filename):\n",
    "            return path+'/'+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "(trainSet.withColumn(\"feature\", vector_to_array(\"catTextFeaturesRobustScaler\")))\\\n",
    ".select([\"target\"]+ [F.col(\"feature\")[i] for i in range(55)]).write.mode('overwrite').csv('trainCSV')\n",
    "\n",
    "(validSet.withColumn(\"feature\", vector_to_array(\"catTextFeaturesRobustScaler\")))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(55)]).write.mode('overwrite').csv('validCSV')\n",
    "\n",
    "trainPath = getCSVFromPysparkPath(\"trainCSV\")\n",
    "validPath = getCSVFromPysparkPath(\"validCSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(testDataPreprocessed.withColumn(\"feature\", vector_to_array(\"catTextFeaturesRobustScaler\")))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(55)]).write.mode('overwrite').csv('testCSV')\n",
    "\n",
    "testPath = getCSVFromPysparkPath(\"testCSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetLgb = lgb.Dataset(trainPath, label_column=0)\n",
    "#validSetLgb = lgb.Dataset(validPath, reference=trainSetLgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 5, \n",
    "         'objective': 'binary',\n",
    "         'metric': 'auc',\n",
    "         'learning_rate': 0.1,\n",
    "         'num_iterations': 500,\n",
    "         'boosting': 'dart'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = lgb.train(param, trainSetLgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predProbModel = bst.predict(validPath, num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plot Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array(prediction.select('prediction').collect()).squeeze()\n",
    "predProb = np.array(prediction.select('probability').collect()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueLabel = np.array(validSet.select('target').collect()).squeeze()\n",
    "predLabel = np.array([1 if x >= 0.5 else 0 for x in predProbModel])\n",
    "predProb = np.array([[x,1.- x] for x in predProbModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(metrics.classification_report(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(trueLabel, predLabel, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(trueLabel, predProb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict and write file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = algorithm.fit(trainingDataPreprocessed).transform(testDataPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabelTest = np.array(predictionTest.select('prediction').collect()).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = bst.predict(testPath, num_iteration=bst.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLabelTest = np.array([1 if x >= 0.5 else 0 for x in predictionTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = submission['target'] + predLabelTest.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"LightGbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
