{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'nlp-getting-started/train.csv'\n",
    "data = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNonNull = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Columns\":20}: {\"All\":10} {\"NonNull\":10} {\"%NonNull\":10} {\"Difference\"}')\n",
    "for idx, col in enumerate(data.columns):\n",
    "    allValue = data.count()[idx]\n",
    "    nonNullValue = dataNonNull.count()[idx]\n",
    "    per = nonNullValue*100/allValue\n",
    "    diff = allValue - nonNullValue\n",
    "    print(f'{col:20}: {allValue} {nonNullValue:10} {np.round(per):10} {diff:10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'].fillna(\"\", inplace=True)\n",
    "data['location'].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'target'\n",
    "\n",
    "#----------------------\n",
    "def getCategoricalColumn(value):\n",
    "    if value == 1: return \"Disaster\"\n",
    "    else: return \"Not disaster\"\n",
    "    \n",
    "CategoricalColumn = data[columnName].apply(getCategoricalColumn)\n",
    "CategoricalColumn.name = 'catTarget'\n",
    "\n",
    "df = pd.concat([data, CategoricalColumn], axis=1)\n",
    "#----------------------\n",
    "\n",
    "groups = []\n",
    "for group, subset in df.groupby(by=CategoricalColumn.name):\n",
    "    groups.append({\n",
    "        CategoricalColumn.name: group,\n",
    "        'Count': len(subset)\n",
    "    })\n",
    "\n",
    "lenData = data[columnName].count()\n",
    "\n",
    "dataCategoricalQuality = pd.DataFrame(groups)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "dataCategoricalQuality.plot.bar(x=CategoricalColumn.name, ax=ax)\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    value = str(groups[i]['Count'])+': '+str(np.round(groups[i]['Count']*100/lenData))+'%'\n",
    "    ax.text(i, groups[i]['Count'], value , horizontalalignment='center', \n",
    "            verticalalignment='bottom')\n",
    "\n",
    "ax.set_ylim(0, lenData - lenData/5)\n",
    "\n",
    "ax.set_xlabel('target')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Sum: '+ str(lenData) )\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].str.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keyword'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Keywords for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target vs keyword length (character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywordLengthChar'] = data['keyword'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'keywordLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that keyword lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'location'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Disaster')\n",
    "crossTable.sort_values(by='Disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most frequent Locations for Not disaster')\n",
    "crossTable.sort_values(by='Not disaster', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target vs text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of character (including space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLengthChar'] = data['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthChar'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that text lengths are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthChar'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordTextLength(text):\n",
    "    return len(text.split())\n",
    "data['textLengthWord'] = data['text'].apply(getWordTextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnName = 'textLengthWord'\n",
    "\n",
    "ax = (data[columnName]).plot.box(figsize=(3, 4))\n",
    "ax.set_ylabel(columnName)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "g = sns.FacetGrid(data, col=columnNameA)\n",
    "g.map(sns.histplot, columnNameB, bins=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that number of words in text are longer in Not Disaster than in Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'textLengthWord'\n",
    "\n",
    "sns.boxplot(data=data, x=columnNameA, y=columnNameB)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(https?://\\S+)'\n",
    "data['link']= data[\"text\"].str.extract(pattern)\n",
    "    \n",
    "data['containLink'] = data['link'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameA = 'target'\n",
    "columnNameB = 'containLink'\n",
    "\n",
    "crossTable = pd.crosstab(index=data[columnNameB],\n",
    "                         columns=data[columnNameA],\n",
    "                         margins=True)\n",
    "\n",
    "crossTable.rename(columns={0 : 'Not disaster',1 : 'Disaster',}, inplace=True)\n",
    "crossTable.rename(index={False : 'No link',True : 'Link',}, inplace=True)\n",
    "\n",
    "crossTable['Not disaster %'] = crossTable['Not disaster'] * 100 / crossTable['All']\n",
    "crossTable['Disaster %'] = crossTable['Disaster'] * 100 / crossTable['All']\n",
    "\n",
    "crossTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are more links in Disaster than Not Disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Processing data using PyPark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler\n",
    "\n",
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")\n",
    "\n",
    "preprocessingPipeline = Pipeline(stages=[removeUrlTransformer, \n",
    "                                         regexTokenizer, stopWordsRemover, word2Vec \n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predict and write file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = algorithm.fit(trainDataPreprocessed).transform(testDataPreprocessed)\n",
    "predLabelTest = np.array(predictionTest.select('prediction').collect()).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### For other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionTest = modelLgb.predict(testPath, num_iteration=modelLgb.best_iteration)\n",
    "predLabelTest = np.array([1 if x >= 0.5 else 0 for x in predictionTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = submission['target'] + predLabelTest.astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"LightGbm-Concat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
