{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark: Natural Language Processing with Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fae1819e6d0>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml:0.9.1\") \\\n",
    "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row in Training: 7613\n",
      "Number of row in Test:     3263\n"
     ]
    }
   ],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "testPath = 'nlp-getting-started/test.csv'\n",
    "\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)\n",
    "\n",
    "print('Number of row in Training:', trainData.count())\n",
    "print('Number of row in Test:    ', testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only create several new features from existing features. Adding more new features clearly helps to understand more the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing Null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a custom transformer to replace Null. In the case of \"keyword\" and \"location\", we replace null value by a symbole e.g. \"\\$\" not an empty string \"\" since OneHotEncoder has an error with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillNanTransformer = FillNanTransformer(inputCols=[\"keyword\", \"location\"], nanReplacement=\"$\")\n",
    "textFillNanTransformer = FillNanTransformer(inputCols=[\"text\"], nanReplacement=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing urls in \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove urls from \"text\" and create a new column to verify if a tex contains an url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class CheckPatternTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(CheckPatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1.).otherwise(0.))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "checkUrlTransformer = CheckPatternTransformer(inputCol=\"text\", outputCol=\"textIsContainedUrl\", pattern=\"(https?://\\S+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get lengths for \"keyword\" and \"text\" (without urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLengthTransformer(Transformer, HasInputCols, HasOutputCols):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(GetLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        for inputCol, outputCol in zip(self.getInputCols(), self.getOutputCols()):\n",
    "            dataset = dataset.withColumn(outputCol, F.length(inputCol))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLengthTransformer = GetLengthTransformer(inputCols=[\"keyword\",\"textNoUrl\"], outputCols=[\"keywordLen\", \"textNoUrlLen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing discrete features: \"keyword\", \"location\" and length features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing \"keyword\" and \"location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\"],\n",
    "                              outputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\"],\n",
    "                              handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing \"text\" (without urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove special characters and stopwords, then use word2vec to obtain a vector of \"textNoUrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate strings in \"keyword\", \"location\", \"textNoUrl\" and then apply the same procedure as for \"textNoUrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCol=None):\n",
    "        super(ConcatenateTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCols()[0]))\n",
    "        for colName in self.getInputCols()[1:]:\n",
    "            dataset = dataset.withColumn(self.getOutputCol(), \n",
    "                F.concat_ws('@', F.col(self.getOutputCol()), F.col(colName)))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatStringTransformer = ConcatenateTransformer(inputCols=[\"keyword\", \"location\", \"textNoUrl\"], outputCol=\"concatString\")\n",
    "concatStringRegexTokenizer = RegexTokenizer(inputCol=\"concatString\", outputCol=\"concatStringArrayWord\", pattern=\"\\\\W\")\n",
    "concatStringStopWordsRemover = StopWordsRemover(inputCol=\"concatStringArrayWord\", outputCol=\"concatStringArrayWordNoSW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like observe effects of removing stopwords by generating two vectors \"concatStringArrayWord\" containing stopwords and \"concatStringArrayWordNoSW\" without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatStringWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWord\", outputCol=\"concatStringVec\")\n",
    "concatStringNoSWWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWordNoSW\", outputCol=\"concatStringNoSWVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combining several features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to observe how these feature sets affect the performance. Note that we scale these feature sets by using RobustScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"discreteFeatures\"**: we only use discrete features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteFeaturesAssembler = VectorAssembler(inputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\",\n",
    "                                                      \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                            outputCol=\"discreteFeatures\")\n",
    "\n",
    "\n",
    "discreteFeaturesRobustScaler = RobustScaler(inputCol=\"discreteFeatures\", outputCol=\"discreteFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"discreteAndTextFeatures\"**: we add features of text in \"discreteFeatures\". Note that this could decay the effects of these discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteAndTextFeaturesAssembler = VectorAssembler(inputCols=[\"discreteFeatures\", \"textVec\"],\n",
    "                                                   outputCol=\"discreteAndTextFeatures\")\n",
    "\n",
    "discreteAndTextFeaturesRobustScaler = RobustScaler(inputCol=\"discreteAndTextFeatures\", outputCol=\"discreteAndTextFeaturesScale\",\n",
    "                                                   withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combining all preprocessing stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[fillNanTransformer, textFillNanTransformer,\n",
    "                                         removeUrlTransformer, regexTokenizer, stopWordsRemover, word2Vec,\n",
    "                                         keywordIndexer, locationIndexer, checkUrlTransformer, getLengthTransformer, oneHotEncoder,\n",
    "                                         concatStringTransformer, concatStringRegexTokenizer, concatStringStopWordsRemover, \n",
    "                                         concatStringWord2Vec, concatStringNoSWWord2Vec,\n",
    "                                         discreteFeaturesAssembler, \n",
    "                                         discreteAndTextFeaturesAssembler,\n",
    "                                         discreteFeaturesRobustScaler, discreteAndTextFeaturesRobustScaler\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "\n",
    "labelCol = \"target\"\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=labelCol, rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=2406)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Several observations on our feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important features by using RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywordIndex        : 0.18558627096125901\n",
      "locationIndex       : 0.02047601483628147\n",
      "textIsContainedUrl  : 0.38809280396066725\n",
      "keywordLen          : 0.2615534413038534\n",
      "textNoUrlLen        : 0.14429146893793882\n"
     ]
    }
   ],
   "source": [
    "featuresCol = \"discreteIndexFeaturesScale\"\n",
    "\n",
    "discreteIndexFeaturesAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\",\n",
    "                                                            \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                                 outputCol=\"discreteIndexFeatures\")\n",
    "\n",
    "discreteIndexFeaturesRobustScaler = RobustScaler(inputCol=\"discreteIndexFeatures\", outputCol=\"discreteIndexFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)\n",
    "\n",
    "checkSet = discreteIndexFeaturesAssembler.transform(trainSet)\n",
    "checkSet = discreteIndexFeaturesRobustScaler.fit(checkSet).transform(checkSet)\n",
    "\n",
    "featuresImportanceModel = RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol).fit(checkSet)\n",
    "\n",
    "for column in zip(discreteIndexFeaturesAssembler.getInputCols(), list(featuresImportanceModel.featureImportances)):\n",
    "     print(f\"{column[0]:20}: {column[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that containing an url in \"text\" has the highest impact on the target.  \n",
    "*Note that we can generate more features based on n-gram, number of urls, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe scores based on several feature sets:**\n",
    "* \"discreteFeaturesScale\": \"keywordIndex\", \"locationIndex\", \"keywordLength\", \"textNoLinkLength\", \"containLink\"\n",
    "* \"discreteAndTextFeaturesScale\": \"discreteFeaturesScale\" and \"vecText\"\n",
    "* \"vecText\" only\n",
    "* \"concatStringVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing urls), then apply Word2Vec\n",
    "* \"concatStringNoSWVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing urls and stopwords), then apply Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param discreteFeaturesScale                             : 0.58467\n",
      "Param discreteAndTextFeaturesScale                      : 0.69364\n",
      "Param textVec                                           : 0.71249\n",
      "Param concatStringVec                                   : 0.69641\n",
      "Param concatStringNoSWVec                               : 0.7066\n"
     ]
    }
   ],
   "source": [
    "featuresCols = [\"discreteFeaturesScale\", \"discreteAndTextFeaturesScale\", \n",
    "                \"textVec\", \"concatStringVec\", \"concatStringNoSWVec\"]\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"LGBMC\":LightGBMClassifier(featuresCol=featuresCol, labelCol=labelCol)\n",
    "                }\n",
    "\n",
    "for param in featuresCols:\n",
    "    scores = []\n",
    "    for name, algorithm in zip(algorithmList.keys(), algorithmList.values()):\n",
    "        algorithm.setFeaturesCol(param)\n",
    "        prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "        scores.append(evaluator.evaluate(prediction))\n",
    "    print(f'Param {param:50}: {np.round(np.mean(scores), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe that \"textVec\" (\"text\" without urls and stopwords) produces the best result**.  \n",
    "* Adding \"keyword\" and \"location\" into \"text\" does not improve the performance. An explaination is that \"text\" can contain both \"keyword\" and \"location\", adding more these words seems to harm the performance.\n",
    "* Removing stopwords can lead to a better result.  \n",
    "* Using more discrete features can help to gain some more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training on several classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use default hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"textVec\"\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, layers=[50,2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol,),\n",
    "                 \"LGBMC\":LightGBMClassifier(featuresCol=featuresCol, labelCol=labelCol)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  : 0.69002 in 16.081s\n",
      "DTC : 0.69808 in 12.514s\n",
      "RFC : 0.67623 in 12.112s\n",
      "GBTC: 0.71931 in 19.075s\n",
      "MPC : 0.69311 in 12.814s\n",
      "LSVC: 0.69121 in 21.75s\n",
      "LGBMC: 0.7213 in 7.369s\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in zip(algorithmList.keys(), algorithmList.values()):\n",
    "    startTime = time.time()\n",
    "    model = algorithm.fit(trainSet)\n",
    "    prediction = model.transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'{name:4}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that LightGBMClassifier (default hyper-parameters) produces the highest score.  \n",
    "We train LightGBMClassifier with all data, **the score of test set is 0.74410**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-826e1600b307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeaturesCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"textVec\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m lgbmc = LightGBMClassifier(boostingType='dart',\n\u001b[0m\u001b[1;32m      5\u001b[0m                            \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                            \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/folders/5_/grw8d03j7xs_vtptfqyvjn780000gn/T/spark-21915f3b-ab1a-4b0d-ae52-231a5cd26d6a/userFiles-e6fbbac0-855a-4f5a-9a27-fab39c970d26/com.microsoft.azure_synapseml-lightgbm-0.9.1.jar/synapse/ml/lightgbm/LightGBMClassifier.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, baggingFraction, baggingFreq, baggingSeed, binSampleCount, boostFromAverage, boostingType, categoricalSlotIndexes, categoricalSlotNames, chunkSize, defaultListenPort, driverListenPort, dropRate, earlyStoppingRound, featureFraction, featuresCol, featuresShapCol, fobj, improvementTolerance, initScoreCol, isProvideTrainingMetric, isUnbalance, labelCol, lambdaL1, lambdaL2, leafPredictionCol, learningRate, matrixType, maxBin, maxBinByFeature, maxDeltaStep, maxDepth, maxDrop, metric, minDataInLeaf, minGainToSplit, minSumHessianInLeaf, modelString, negBaggingFraction, numBatches, numIterations, numLeaves, numTasks, numThreads, objective, parallelism, posBaggingFraction, predictionCol, probabilityCol, rawPredictionCol, repartitionByGroupingColumn, skipDrop, slotNames, thresholds, timeout, topK, uniformDrop, useBarrierExecutionMode, useSingleDatasetMode, validationIndicatorCol, verbosity, weightCol, xgboostDartMode)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLightGBMClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjava_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.microsoft.azure.synapse.ml.lightgbm.LightGBMClassifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1698\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark_env/lib/python3.8/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    401\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "labelCol = \"target\"\n",
    "featuresCol = \"textVec\"\n",
    "\n",
    "lgbmc = LightGBMClassifier(boostingType='dart',\n",
    "                           objective= 'binary',\n",
    "                           metric= 'auc',\n",
    "                           isUnbalance= True,\n",
    "                           numIterations= 300,\n",
    "                           labelCol=\"target\",\n",
    "                           featuresCol=\"textVec\")\n",
    "\n",
    "prediction = lgbmc.fit(trainSet).transform(validSet)\n",
    "print(evaluator.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning LightGBMClassifier** using TuneHyperparameters  \n",
    "https://microsoft.github.io/SynapseML/docs/documentation/estimators/estimators_core/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to convert our dataframe into a simple datafram with each column representing a feature and one column for target. Other types (e.g. vector, array) can lead to an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.automl import *\n",
    "from synapse.ml.train import *\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "\n",
    "trainSetAllHP = (trainDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "trainSetHP = (trainSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "validSetHP = (validSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "testSetHP = (testDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(50)])\n",
    "\n",
    "\n",
    "# We remove \"[]\" in the column names.\n",
    "trainSetAllHP = trainSetAllHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in trainSetAllHP.columns])\n",
    "trainSetHP = trainSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in trainSetHP.columns])\n",
    "validSetHP = validSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in validSetHP.columns])\n",
    "testSetHP = testSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in testSetHP.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.automl import *\n",
    "from synapse.ml.train import *\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lgbmc = LightGBMClassifier(boostingType='dart',\n",
    "                           objective= 'binary',\n",
    "                           metric= 'auc',\n",
    "                           isUnbalance= True,\n",
    "                           numIterations= 300)\n",
    "\n",
    "smlmodels = [lgbmc]\n",
    "mmlmodels = [TrainClassifier(model=model, labelCol=labelCol) for model in smlmodels]\n",
    "\n",
    "paramBuilder = (HyperparamBuilder()\n",
    ".addHyperparam(lgbmc, lgbmc.learningRate, RangeHyperParam(0.01, 0.5))\n",
    ".addHyperparam(lgbmc, lgbmc.maxDepth, DiscreteHyperParam([1,30]))\n",
    ".addHyperparam(lgbmc, lgbmc.numLeaves, DiscreteHyperParam([10,200]))\n",
    ".addHyperparam(lgbmc, lgbmc.featureFraction, RangeHyperParam(0.1, 1.0))\n",
    ".addHyperparam(lgbmc, lgbmc.baggingFraction, RangeHyperParam(0.1, 1.0))\n",
    ".addHyperparam(lgbmc, lgbmc.baggingFreq, RangeHyperParam(0, 3))\n",
    ")\n",
    "\n",
    "searchSpace = paramBuilder.build()\n",
    "\n",
    "randomSpace = RandomSpace(searchSpace)\n",
    "\n",
    "bestModel = TuneHyperparameters(evaluationMetric=\"AUC\", models=mmlmodels, numFolds=2, \n",
    "                                numRuns=len(mmlmodels) * 2, parallelism=2, \n",
    "                                paramSpace=randomSpace.space(), seed=0).fit(trainSetHP)\n",
    "\n",
    "\n",
    "prediction = bestModel.transform(validSetHP)\n",
    "predLabel = np.array(prediction.select('scored_labels').collect()).squeeze()\n",
    "trueLabel = np.array(prediction.select('target').collect()).squeeze()\n",
    "print(metrics.roc_auc_score(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
