{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark: Natural Language Processing with Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f9d54756730>\n"
     ]
    }
   ],
   "source": [
    "# The config below is for https://microsoft.github.io/SynapseML/\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml:0.9.1\") \\\n",
    "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row in Training: 7613\n",
      "Number of row in Test:     3263\n"
     ]
    }
   ],
   "source": [
    "trainPath = 'nlp-getting-started/train.csv'\n",
    "testPath = 'nlp-getting-started/test.csv'\n",
    "\n",
    "trainData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(trainPath)\n",
    "testData = spark.read.format('csv').options(header='true', inferSchema='true', multiLine=True).load(testPath)\n",
    "\n",
    "print('Number of row in Training:', trainData.count())\n",
    "print('Number of row in Test:    ', testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only create several new features from existing features. Adding more new features clearly helps to understand more the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Pipeline \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer,OneHotEncoder, VectorAssembler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing Null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a custom transformer to replace Null. In the case of \"keyword\" and \"location\", we replace null value by a symbole e.g. \"\\$\" not an empty string \"\" since OneHotEncoder has an error with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNanTransformer(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    nanReplacement = Param(Params._dummy(), \"nanReplacement\", \"nanReplacement\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, nanReplacement=None):\n",
    "        super(FillNanTransformer, self).__init__()\n",
    "        self._setDefault(nanReplacement=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, nanReplacement=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getNanReplacement(self):\n",
    "        return self.getOrDefault(self.nanReplacement)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        nanReplacement = self.getNanReplacement()\n",
    "        dataset = dataset.na.fill(value=nanReplacement,subset=self.getInputCols())\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillNanTransformer = FillNanTransformer(inputCols=[\"keyword\", \"location\"], nanReplacement=\"$\")\n",
    "textFillNanTransformer = FillNanTransformer(inputCols=[\"text\"], nanReplacement=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing urls in \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove urls from \"text\" and create a new column to verify if a tex contains an url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemovePatternTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(RemovePatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.regexp_replace(F.col(self.getInputCol()), pattern, \"\"))\n",
    "        return dataset\n",
    "    \n",
    "class CheckPatternTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    pattern = Param(Params._dummy(), \"pattern\", \"pattern\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        super(CheckPatternTransformer, self).__init__()\n",
    "        self._setDefault(pattern=\"\")\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, pattern=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def getPattern(self):\n",
    "        return self.getOrDefault(self.pattern)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        pattern = self.getPattern()\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.when(F.col(self.getInputCol()).rlike(pattern),1.).otherwise(0.))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeUrlTransformer = RemovePatternTransformer(inputCol=\"text\", outputCol=\"textNoUrl\", pattern=\"(https?://\\S+)\")\n",
    "checkUrlTransformer = CheckPatternTransformer(inputCol=\"text\", outputCol=\"textIsContainedUrl\", pattern=\"(https?://\\S+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get lengths for \"keyword\" and \"text\" (without urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLengthTransformer(Transformer, HasInputCols, HasOutputCols):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCols=None):\n",
    "        super(GetLengthTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCols=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        for inputCol, outputCol in zip(self.getInputCols(), self.getOutputCols()):\n",
    "            dataset = dataset.withColumn(outputCol, F.length(inputCol))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLengthTransformer = GetLengthTransformer(inputCols=[\"keyword\",\"textNoUrl\"], outputCols=[\"keywordLen\", \"textNoUrlLen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing discrete features: \"keyword\", \"location\" and length features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing \"keyword\" and \"location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordIndexer = StringIndexer(inputCol=\"keyword\", outputCol=\"keywordIndex\", handleInvalid=\"keep\")\n",
    "locationIndexer = StringIndexer(inputCol=\"location\", outputCol=\"locationIndex\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing \"text\" (without urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove special characters and stopwords, then use word2vec to obtain a vector of \"textNoUrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"textNoUrl\", outputCol=\"textArrayWord\", pattern=\"\\\\W\")\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"textArrayWord\", outputCol=\"textNoSW\")\n",
    "word2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"textNoSW\", outputCol=\"textVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate strings in \"keyword\", \"location\", \"textNoUrl\" and then apply the same procedure as for \"textNoUrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateTransformer(Transformer, HasInputCols, HasOutputCol):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCols=None, outputCol=None):\n",
    "        super(ConcatenateTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCols=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCols()[0]))\n",
    "        for colName in self.getInputCols()[1:]:\n",
    "            dataset = dataset.withColumn(self.getOutputCol(), \n",
    "                F.concat_ws('@', F.col(self.getOutputCol()), F.col(colName)))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatStringTransformer = ConcatenateTransformer(inputCols=[\"keyword\", \"location\", \"textNoUrl\"], outputCol=\"concatString\")\n",
    "concatStringRegexTokenizer = RegexTokenizer(inputCol=\"concatString\", outputCol=\"concatStringArrayWord\", pattern=\"\\\\W\")\n",
    "concatStringStopWordsRemover = StopWordsRemover(inputCol=\"concatStringArrayWord\", outputCol=\"concatStringArrayWordNoSW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like observe effects of removing stopwords by generating two vectors \"concatStringArrayWord\" containing stopwords and \"concatStringArrayWordNoSW\" without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatStringWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWord\", outputCol=\"concatStringVec\")\n",
    "concatStringNoSWWord2Vec = Word2Vec(vectorSize=50, windowSize=10, minCount=0, inputCol=\"concatStringArrayWordNoSW\", outputCol=\"concatStringNoSWVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combining several features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to observe how these feature sets affect the performance. Note that we scale these feature sets by using RobustScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"discreteFeatures\"**: we only use discrete features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteFeaturesAssembler = VectorAssembler(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\",\n",
    "                                                      \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                            outputCol=\"discreteFeatures\")\n",
    "\n",
    "\n",
    "discreteFeaturesRobustScaler = RobustScaler(inputCol=\"discreteFeatures\", outputCol=\"discreteFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"discreteOneHotEncoderFeatures\"**: we use these discrete features one-hot-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(inputCols=[\"keywordIndex\", \"locationIndex\", \"textIsContainedUrl\"],\n",
    "                              outputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\"],\n",
    "                              handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteOneHotEncoderFeaturesAssembler = VectorAssembler(inputCols=[\"keywordVec\", \"locationVec\", \"textIsContainedUrlVec\",\n",
    "                                                      \"keywordLen\", \"textNoUrlLen\"], \n",
    "                                            outputCol=\"discreteOneHotEncoderFeatures\")\n",
    "\n",
    "\n",
    "discreteOneHotEncoderFeaturesRobustScaler = RobustScaler(inputCol=\"discreteOneHotEncoderFeatures\", outputCol=\"discreteOneHotEncoderFeaturesScale\",\n",
    "                                            withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"discreteAndTextFeatures\"**: we add features of text in \"discreteFeatures\". Note that this could decay the effects of these discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteAndTextFeaturesAssembler = VectorAssembler(inputCols=[\"discreteFeatures\", \"textVec\"],\n",
    "                                                   outputCol=\"discreteAndTextFeatures\")\n",
    "\n",
    "discreteAndTextFeaturesRobustScaler = RobustScaler(inputCol=\"discreteAndTextFeatures\", outputCol=\"discreteAndTextFeaturesScale\",\n",
    "                                                   withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"discreteOneHotEncoderAndTextFeatures\"**: we add features of text in \"discreteOneHotEncoderFeatures\". Note that this could decay the effects of these discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteOneHotEncoderAndTextFeaturesAssembler = VectorAssembler(inputCols=[\"discreteOneHotEncoderFeatures\", \"textVec\"],\n",
    "                                                   outputCol=\"discreteOneHotEncoderAndTextFeatures\")\n",
    "\n",
    "discreteOneHotEncoderAndTextFeaturesRobustScaler = RobustScaler(inputCol=\"discreteOneHotEncoderAndTextFeatures\", outputCol=\"discreteOneHotEncoderAndTextFeaturesScale\",\n",
    "                                                   withScaling=True, withCentering=True, lower=0.25, upper=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combining all preprocessing stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingPipeline = Pipeline(stages=[fillNanTransformer, textFillNanTransformer,\n",
    "                                         removeUrlTransformer, regexTokenizer, stopWordsRemover, word2Vec,\n",
    "                                         keywordIndexer, locationIndexer, checkUrlTransformer, getLengthTransformer, \n",
    "                                         #oneHotEncoder,\n",
    "                                         #concatStringTransformer, concatStringRegexTokenizer, concatStringStopWordsRemover, \n",
    "                                         #concatStringWord2Vec, concatStringNoSWWord2Vec,\n",
    "                                         discreteFeaturesAssembler, \n",
    "                                         #discreteOneHotEncoderFeaturesAssembler, \n",
    "                                         discreteAndTextFeaturesAssembler,\n",
    "                                         #discreteOneHotEncoderAndTextFeaturesAssembler,\n",
    "                                         discreteFeaturesRobustScaler, \n",
    "                                         #discreteOneHotEncoderFeaturesRobustScaler,\n",
    "                                         discreteAndTextFeaturesRobustScaler,\n",
    "                                         #discreteOneHotEncoderAndTextFeaturesRobustScaler\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "\n",
    "labelCol = \"target\"\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=labelCol, rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingModel = preprocessingPipeline.fit(trainData)\n",
    "\n",
    "trainDataPreprocessed = preprocessingModel.transform(trainData)\n",
    "testDataPreprocessed = preprocessingModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Several observations on our feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important features by using RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywordIndex        : 0.17150511557605724\n",
      "locationIndex       : 0.019880210358211636\n",
      "textIsContainedUrl  : 0.41729726391614824\n",
      "keywordLen          : 0.2596097385419455\n",
      "textNoUrlLen        : 0.13170767160763733\n"
     ]
    }
   ],
   "source": [
    "featuresCol = \"discreteFeaturesScale\"\n",
    "\n",
    "featuresImportanceModel = RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol).fit(trainDataPreprocessed)\n",
    "\n",
    "for column in zip(discreteFeaturesAssembler.getInputCols(), list(featuresImportanceModel.featureImportances)):\n",
    "     print(f\"{column[0]:20}: {column[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that containing an url in \"text\" has the highest impact on the target.  \n",
    "*Note that we can generate more features based on n-gram, number of urls, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe scores based on several feature sets:**\n",
    "* \"discreteFeaturesScale\": \"keywordIndex\", \"locationIndex\", \"keywordLen\", \"textNoUrlLen\", \"textIsContainedUrl\"\n",
    "* \"discreteOneHotEncoderFeaturesScale\": \"discreteFeatures\" after one-hot-encoding\n",
    "* \"vecText\": vector of \"text\" (not containing urls) using Word2Vec\n",
    "* \"discreteAndTextFeaturesScale\": \"discreteFeatures\" and \"vecText\"\n",
    "* \"discreteOneHotEncoderAndTextFeaturesScale\": \"discreteOneHotEncoderFeatures\" and \"vecText\"\n",
    "* \"concatStringVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing urls), then apply Word2Vec\n",
    "* \"concatStringNoSWVec\": concatenate \"keyword\", \"location\" and \"text\" (not containing urls and stopwords), then apply Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param discreteFeaturesScale                             : 0.67728\n",
      "Param discreteOneHotEncoderFeaturesScale                : 0.6132\n",
      "Param textVec                                           : 0.71302\n",
      "Param discreteAndTextFeaturesScale                      : 0.73\n",
      "Param discreteOneHotEncoderAndTextFeaturesScale         : 0.69689\n",
      "Param concatStringVec                                   : 0.729\n",
      "Param concatStringNoSWVec                               : 0.71234\n"
     ]
    }
   ],
   "source": [
    "featuresCols = [\"discreteFeaturesScale\", \"discreteOneHotEncoderFeaturesScale\", \"textVec\", \"discreteAndTextFeaturesScale\", \n",
    "                \"discreteOneHotEncoderAndTextFeaturesScale\", \"concatStringVec\", \"concatStringNoSWVec\"]\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(labelCol=labelCol),\n",
    "                 \"DTC\":  DecisionTreeClassifier(labelCol=labelCol),\n",
    "                 \"RFC\":  RandomForestClassifier(labelCol=labelCol),\n",
    "                 \"GBTC\": GBTClassifier(labelCol=labelCol),\n",
    "                 \"LSVC\": LinearSVC(labelCol=labelCol),\n",
    "                 \"LGBMC\":LightGBMClassifier(labelCol=labelCol)\n",
    "                }\n",
    "\n",
    "from random import randint\n",
    "for param in featuresCols:\n",
    "    scores = []\n",
    "    for name, algorithm in zip(algorithmList.keys(), algorithmList.values()):\n",
    "        trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=randint(11, 2021))\n",
    "        algorithm.setFeaturesCol(param)\n",
    "        prediction = algorithm.fit(trainSet).transform(validSet)\n",
    "        scores.append(evaluator.evaluate(prediction))\n",
    "    print(f'Param {param:50}: {np.round(np.mean(scores), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe that \"discreteAndTextFeaturesScale\" produces the best result.**  \n",
    "* Using one-hot-encoder harms the performance in this situation.\n",
    "* Removing stopwords reduces the score, which means that these stopwords can help to decide if a tweet is about a disaster.\n",
    "* Using more discrete features can help to gain some more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training on several classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, validSet = trainDataPreprocessed.randomSplit([0.9, 0.1], seed=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use default hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = \"discreteAndTextFeaturesScale\"\n",
    "\n",
    "algorithmList = {\"LR\":   LogisticRegression(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"DTC\":  DecisionTreeClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"RFC\":  RandomForestClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"GBTC\": GBTClassifier(featuresCol=featuresCol, labelCol=labelCol),\n",
    "                 \"MPC\":  MultilayerPerceptronClassifier(featuresCol=featuresCol, labelCol=labelCol, layers=[55,2]),\n",
    "                 \"LSVC\": LinearSVC(featuresCol=featuresCol, labelCol=labelCol,),\n",
    "                 \"LGBMC\":LightGBMClassifier(featuresCol=featuresCol, labelCol=labelCol)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  : 0.72925 in 12.207s\n",
      "DTC : 0.67405 in 11.687s\n",
      "RFC : 0.6984 in 11.648s\n",
      "GBTC: 0.73299 in 20.61s\n",
      "MPC : 0.73037 in 12.813s\n",
      "LSVC: 0.71278 in 19.878s\n",
      "LGBMC: 0.76522 in 7.099s\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in zip(algorithmList.keys(), algorithmList.values()):\n",
    "    startTime = time.time()\n",
    "    model = algorithm.fit(trainSet)\n",
    "    prediction = model.transform(validSet)\n",
    "    score = evaluator.evaluate(prediction)\n",
    "    print(f'{name:4}: {np.round(score,5)} in {np.round(time.time() - startTime, 3)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that LightGBMClassifier (default hyper-parameters) produces the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train LightGBMClassifier with all data, **the score of test set is 0.74410**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning LightGBMClassifier** using TuneHyperparameters  \n",
    "https://microsoft.github.io/SynapseML/docs/documentation/estimators/estimators_core/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to convert our dataframe into a simple datafram with each column representing a feature and one column for target. Other types (e.g. vector, array) can lead to an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.automl import *\n",
    "from synapse.ml.train import *\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "\n",
    "featuresCol = \"discreteAndTextFeaturesScale\"\n",
    "\n",
    "trainSetAllHP = (trainDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(55)])\n",
    "\n",
    "trainSetHP = (trainSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(55)])\n",
    "\n",
    "validSetHP = (validSet.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([labelCol]+ [F.col(\"feature\")[i] for i in range(55)])\n",
    "\n",
    "testSetHP = (testDataPreprocessed.withColumn(\"feature\", vector_to_array(featuresCol)))\\\n",
    ".select([F.col(\"feature\")[i] for i in range(55)])\n",
    "\n",
    "\n",
    "# We remove \"[]\" in the column names.\n",
    "trainSetAllHP = trainSetAllHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in trainSetAllHP.columns])\n",
    "trainSetHP = trainSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in trainSetHP.columns])\n",
    "validSetHP = validSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in validSetHP.columns])\n",
    "testSetHP = testSetHP.select([F.col(col).alias(re.sub(\"[^0-9a-zA-Z$]+\",\"\",col)) for col in testSetHP.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = true)\n",
      " |-- feature0: double (nullable = true)\n",
      " |-- feature1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSetHP.select(\"target\", \"feature0\", \"feature1\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.automl import *\n",
    "from synapse.ml.train import *\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lgbmc = LightGBMClassifier(boostingType='dart',\n",
    "                           objective= 'binary',\n",
    "                           metric= 'auc',\n",
    "                           isUnbalance= True,\n",
    "                           numIterations= 300)\n",
    "\n",
    "smlmodels = [lgbmc]\n",
    "mmlmodels = [TrainClassifier(model=model, labelCol=labelCol) for model in smlmodels]\n",
    "\n",
    "paramBuilder = (HyperparamBuilder()\n",
    ".addHyperparam(lgbmc, lgbmc.learningRate, RangeHyperParam(0.01, 0.5))\n",
    ".addHyperparam(lgbmc, lgbmc.maxDepth, DiscreteHyperParam([1,30]))\n",
    ".addHyperparam(lgbmc, lgbmc.numLeaves, DiscreteHyperParam([10,200]))\n",
    ".addHyperparam(lgbmc, lgbmc.featureFraction, RangeHyperParam(0.1, 1.0))\n",
    ".addHyperparam(lgbmc, lgbmc.baggingFraction, RangeHyperParam(0.1, 1.0))\n",
    ".addHyperparam(lgbmc, lgbmc.baggingFreq, RangeHyperParam(0, 3))\n",
    ")\n",
    "\n",
    "searchSpace = paramBuilder.build()\n",
    "\n",
    "randomSpace = RandomSpace(searchSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = TuneHyperparameters(evaluationMetric=\"AUC\", models=mmlmodels, numFolds=2, \n",
    "                                numRuns=len(mmlmodels) * 2, parallelism=1, \n",
    "                                paramSpace=randomSpace.space(), seed=0).fit(trainSetHP)\n",
    "\n",
    "prediction = bestModel.transform(validSetHP)\n",
    "predLabel = np.array(prediction.select('scored_labels').collect()).squeeze()\n",
    "trueLabel = np.array(prediction.select('target').collect()).squeeze()\n",
    "print(metrics.roc_auc_score(trueLabel, predLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The score of test set is 0.77536.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, we showed several steps in processing text and the performance of several models implemented in Pyspark.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
